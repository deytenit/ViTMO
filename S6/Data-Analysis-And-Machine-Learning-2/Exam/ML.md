# Машинное обучение 2025

| Left Column                                                                 | Right Column                                                             |
|----------------------------------------------------------------------------|---------------------------------------------------------------------------|
| [1. Матричное преобразование](#1-матричное-преобразование)                | [23. Преобразование Фурье](#23-преобразование-фурье)                     |
| [2. Функции активации](#2-функции-активации)                              | [24. Спектрограммы Мела. MFCC.](#24-спектрограммы-мела-mfcc)            |
| [3. Остаточные связи](#3-остаточные-связи)                                | [25. DeepSets](#25-deepsets)                                             |
| [4. Сети Колмогорова – Арнольда](#4-сети-колмогорова--арнольда)          | [26. Передача сообщений и случайное блуждание](#26-передача-сообщений-и-случайное-блуждание) |
| [5. Дообучение](#5-дообучение)                                            | [27. Самообучение](#27-самообучение)                                     |
| [6. Дистилляция знаний](#6-дистилляция-знаний)                            | [28. Автокодировщик](#28-автокодировщик)                                 |
| [7. Перенос знаний](#7-перенос-знаний)                                    | [29. Контрастное обучение](#29-контрастное-обучение)                     |
| [8. Квантизация](#8-квантизация)                                          | [30. Word2Vec](#30-word2vec)                                             |
| [9. Дропаут](#9-дропаут)                                                  | [31. Авторегрессионная генерация](#31-авторегрессионная-генерация)     |
| [10. Пакетная нормализация](#10-пакетная-нормализация)                    | [32. Beam Search](#32-beam-search)                                       |
| [11. Инициализация параметров методы Xavier и He](#11-инициализация-параметров-методы-xavier-и-he) | [33. Вариационный автокодировщик](#33-вариационный-автокодировщик)    |
| [12. Статический и динамический граф вычислений](#12-статический-и-динамический-граф-вычислений) | [34. Генеративно-состязательные сети](#34-генеративно-состязательные-сети) |
| [13. Прямое и обратное дифференцирование](#13-прямое-и-обратное-дифференцирование) | [35. Диффузная модель](#35-диффузная-модель)                       |
| [14. Импульсный и адаптивный градиентный спуск](#14-импульсный-и-адаптивный-градиентный-спуск) | [36. point- pair- и list-wise …](#36-point--pair--и-list-wise-подходы-к-рекомендациям) |
| [15. Двумерная свёртка. Пулинг.](#15-двумерная-свёртка-пулинг)            | [37. Разложение матриц (ALS)](#37-разложение-матриц-als)                |
| [16. Семантическая сегментация. Локализация. …](#16-семантическая-сегментация-локализация-детекция-сегментация-объектов) | [38. Самообучение и сообучение](#38-самообучение-self-training-и-сообучение-co-training) |
| [17. UNET. R-CNN. YOLO.](#17-unet-r-cnn-yolo)                             | [39. Активное обучение](#39-активное-обучение)                           |
| [18. Рекуррентные сети](#18-рекуррентные-сети)                            | [40. Задача о многоруком бандите](#40-задача-о-многоруком-бандите)      |
| [19. LSTM](#19-lstm)                                                      | [41. Байесовская оптимизация](#41-байесовская-оптимизация)              |
| [20. Трансформеры](#20-трансформеры)                                      | [42. Обучение с подкреплением](#42-обучение-с-подкреплением)            |
| [21. Позиционное кодирование](#21-позиционное-кодирование)                | [43. Уравнение Беллмана](#43-уравнение-беллмана)                         |
| [22. Частота Найквиста](#22-частота-найквиста)                            |                                                                           |


## [1. Матричное преобразование.](#Машинное-обучение-2025)

Согласно конспекту, **матричное преобразование** — это базовый обучаемый блок в многослойных нейронных сетях. Оно состоит из трёх последовательных операций: умножение входного вектора на матрицу весов, прибавление вектора-сдвига и применение нелинейной функции активации. Это преобразование является основой любого "слоя" в классических нейросетях.

Его можно описать следующей формулой из **Лекции 1**:

$Y_i = f(X_i \cdot A_i + b_i)$

Где:
*   $X_i$ — входной вектор (или пакет векторов).
*   $A_i$ — матрица параметров, называемых **"весами"**.
*   $b_i$ — вектор параметров, называемый **"сдвигом"**.
*   $f$ (или $\sigma$) — нелинейная **функция активации** (например, ReLU или сигмоида).

Основная цель матричного преобразования — изменять размерность векторов и извлекать признаки из данных. Как отмечается в **Лекции 1**, без нелинейной функции активации любая последовательность матричных преобразований была бы эквивалентна одному, что сильно ограничило бы выразительную способность модели. В современных архитектурах, таких как **Трансформеры (Лекция 8)**, матричные преобразования используются для проецирования входных векторов в пространства Запроса (Query), Ключа (Key) и Значения (Value) с помощью отдельных весовых матриц: $Q = XW_Q, K = XW_K, V = XW_V$.

## [2. Функции активации.](#Машинное-обучение-2025)

Согласно **Лекции 1**, **функция активации** — это обязательный нелинейный компонент базового обучаемого преобразования в нейронных сетях. Она применяется после матричного умножения и сдвига, чтобы модель могла изучать сложные, нелинейные зависимости в данных. Без нелинейности последовательность слоёв была бы эквивалентна одному линейному преобразованию.

В конспекте (**Лекция 1**) рассматриваются следующие функции активации:

### Классические функции активации

| Функция | Формула (из Лекции 1) | Свойства (преимущества и недостатки) |
| :--- | :--- | :--- |
| **Сигмоида** | $\sigma(x) = \frac{1}{1 + e^{-x}}$ | **Недостаток:** Вызывает проблему затухания/взрыва градиентов в глубоких сетях. |
| **Гиперболический тангенс (tanh)** | *(формула не приведена)* | **Преимущества:** Диапазон вывода `[-1, +1]` даёт большую "силу" градиентам и меньшую смещённость по сравнению с сигмоидой. |
| **ReLU (Rectified Linear Unit)** | $max(0, x)$ | **Преимущества:** Быстро вычисляется, нет насыщения градиента, легко обучается.<br>**Недостатки:** Несимметрична; существует проблема "умирания ReLU", когда некоторые нейроны перестают активироваться из-за больших градиентов. |

### Вариации ReLU

Для решения проблем ReLU были предложены её модификации:
*   **Leaky ReLU:** Допускает небольшой отрицательный наклон, чтобы нейроны не "умирали".
*   **Parametric ReLU (PReLU):** Делает этот наклон обучаемым параметром.
*   **Noisy ReLU:** Добавляет гауссовский шум.
*   Другие гладкие варианты, такие как **ELU, Softplus, Swish, Mish**.

Выбор функции активации, как указано в лекции, часто основывается на эмпирических результатах. Кроме того, в **Лекции 2** отмечается, что для разных активаций подходят разные методы инициализации весов: **Xavier** для `tanh` и **He** для `ReLU`, что помогает бороться с проблемой затухания/взрыва градиентов.

## [3. Остаточные связи.](#Машинное-обучение-2025)

Согласно **Лекции 1**, **остаточные связи** (также называемые **пропускными соединениями** или **skip connections**) — это архитектурный приём, который решает проблему **деградации** в глубоких нейронных сетях. Проблема заключается в том, что простое добавление новых слоёв не всегда улучшает результат и может даже ухудшать его. Остаточные связи создают "короткий путь" для данных и градиентов, позволяя им обходить один или несколько слоёв. Как отмечается в **Лекции 5**, это также помогает бороться с проблемой затухания/взрыва градиентов. Благодаря этому механизму стало возможным строить и эффективно обучать очень глубокие модели. В конспекте указано, что остаточные связи являются ключевым элементом таких известных архитектур, как **ResNet** и **GoogLeNet**.

## [4. Сети Колмогорова – Арнольда.](#Машинное-обучение-2025)

Согласно **Лекции 1**, **Сети Колмогорова–Арнольда (Kolmogorov-Arnold Networks, KAN)** — это архитектура нейронных сетей, основанная на **теореме Колмогорова-Арнольда**. Эта теорема гласит, что любую непрерывную функцию многих переменных можно представить в виде суммы одномерных функций. В отличие от традиционных многослойных перцептронов (MLP), которые строят сложную функцию путем композиции простых нелинейностей (функций активации) и линейных преобразований (весов), KAN реализует другой подход: сами одномерные функции на рёбрах вычислительного графа являются обучаемыми. Для их аппроксимации могут использоваться, например, **B-сплайны**.

Ключевое различие между MLP и KAN, согласно конспекту, можно представить в виде таблицы:

| Характеристика | Многослойный перцептрон (MLP) | Сеть Колмогорова–Арнольда (KAN) |
| :--- | :--- | :--- |
| **Основа** | Композиция линейных преобразований и фиксированных нелинейностей. | Сумма обучаемых одномерных функций. |
| **Обучаемые параметры** | Веса в матрицах линейных преобразований. | Параметры, определяющие форму одномерных функций (например, коэффициенты сплайнов). |
| **Расположение нелинейности**| В узлах (нейронах) графа. | На рёбрах графа. |

Одним из преимуществ KAN, отмеченных в конспекте, является потенциально более высокая **интерпретируемость** модели, так как можно визуализировать и анализировать выученные одномерные функции.

## [5. Дообучение.](#Машинное-обучение-2025)

Согласно конспектам, **дообучение** (fine-tuning) — это процесс адаптации предварительно обученной нейронной сети к новой, специфической задаче. Этот подход является ключевым элементом **переноса знаний (transfer learning)** и позволяет значительно экономить вычислительные ресурсы и время, используя уже извлечённые базовые признаки.

В лекциях описаны следующие методы и аспекты дообучения:

*   **Классическое дообучение ("Заморозка-Разморозка", Лекции 1, 11):**
    Часть слоёв предобученной модели "замораживается", то есть их веса не обновляются. Обучаются только новые, добавленные слои. Затем, на последующих этапах, можно "разморозить" некоторые из исходных слоёв и дообучать их с меньшей скоростью обучения (learning rate) для тонкой подстройки под новую задачу.

*   **Параметр-эффективное дообучение (LoRA, Лекция 1):**
    **Low-Rank Adaptation (LoRA)** — это современный метод, позволяющий избежать дообучения всех параметров большой модели. Вместо этого обучаются только две небольшие (низкоранговые) матрицы, которые представляют изменение исходных весов. Это значительно снижает требования к памяти и вычислениям, что особенно важно для крупных моделей.

*   **Дообучение для LLM ("Выравнивание", Лекция 8):**
    Для больших языковых моделей (LLM) дообучение используется для "выравнивания" (alignment) — процесса, в ходе которого модель учат давать более полезные, безопасные и релевантные ответы, соответствующие ожиданиям человека. Это часто делается с помощью методов вроде обучения с подкреплением на основе человеческой обратной связи (RLHF).

*   **Самообучение как основа для дообучения (Лекция 11):**
    Подход самообучения (self-supervised learning) используется для создания мощных предобученных моделей на огромных неразмеченных данных (например, на задаче предсказания поворота изображения). Затем эти модели дообучаются на небольшой размеченной выборке для решения целевой ("downstream") задачи, например, классификации.

## [6. Дистилляция знаний.](#Машинное-обучение-2025)

Согласно **Лекции 2** и **Лекции 11**, **дистилляция знаний** — это техника переноса "знаний" от большой, сложной и, как правило, медленной модели (**"учитель"**) к более компактной и быстрой модели (**"студент"**).

Основная идея заключается в том, что "студент" обучается не на исходных "жёстких" метках данных (например, 0 или 1), а на "мягких" метках, которые представляют собой выходы модели-"учителя" (например, распределение вероятностей). Таким образом, "студент" учится имитировать поведение "учителя", включая его "неуверенность" и обобщающую способность, а не просто заучивать правильные ответы.

Ключевые цели дистилляции знаний, указанные в **Лекции 2**:
*   **Уменьшить размер модели.**
*   **Ускорить инференс** (работу модели на новых данных).
*   **Сохранить хорошее качество** предсказаний, близкое к качеству модели-"учителя".

## [7. Перенос знаний.](#Машинное-обучение-2025)

Согласно конспектам, **перенос знаний (transfer learning)** — это методология машинного обучения, при которой модель, обученная на одной (обычно большой и общей) задаче, используется в качестве отправной точки для решения другой, более специфической задачи. Этот подход позволяет использовать уже извлечённые знания о базовых признаках и структурах данных, что особенно эффективно, когда для целевой задачи мало размеченных данных.

В конспектах (**Лекция 1, 11**) описан следующий типичный сценарий переноса знаний:

1.  **Предобучение (Pre-training):** Большая нейронная сеть обучается на масштабном наборе данных (например, ImageNet для изображений или большие текстовые корпуса для языковых моделей). Часто для этого используется **самообучение (self-supervised learning)**, где метки генерируются автоматически (например, предсказание замаскированного слова в тексте, как в BERT, из **Лекции 8**).
2.  **Дообучение (Fine-tuning):** Предобученная модель (или её часть) адаптируется к новой, целевой ("downstream") задаче.

Методы реализации переноса знаний, упомянутые в лекциях:

*   **Использование как извлекатель признаков:** Предобученная часть модели используется для получения векторных представлений (признаков), на которых затем обучается простой классификатор. При этом веса предобученной части "заморожены" и не меняются (**Лекция 11**).
*   **Дообучение с "заморозкой-разморозкой":** Сначала обучаются только новые, добавленные слои, а потом часть или все слои исходной модели "размораживаются" для тонкой подстройки под новую задачу, часто с меньшей скоростью обучения (**Лекция 1**).
*   **LoRA (Low-Rank Adaptation):** Эффективный метод дообучения, при котором модифицируются не все веса большой модели, а только небольшие "адаптеры", что значительно экономит ресурсы (**Лекция 1**).

Перенос знаний — это фундаментальная концепция, лежащая в основе успеха многих современных моделей, включая большие языковые модели (LLM). Стоит отличать его от **дистилляции знаний**, которая заключается в переносе знаний от большой модели к маленькой для сжатия, а не для адаптации к новой задаче.

## [8. Квантизация.](#Машинное-обучение-2025)

Согласно **Лекции 2**, **квантизация** — это процесс представления весов и активаций нейронной сети числами с меньшим числом бит. Например, это может быть переход от стандартного 32-битного представления с плавающей точкой (float32) к 16-битному (float16) или 8-битному целочисленному (int8). Основная цель квантизации — оптимизация модели для более эффективного развертывания (инференса) за счет:

*   **Улучшения производительности** и **ускорения вычислений**, так как операции с числами меньшей точности выполняются быстрее.
*   **Снижения требований к памяти**, что критично для запуска моделей на устройствах с ограниченными ресурсами.

В конспекте также отмечается, что квантизация бывает **симметричной и несимметричной**, а обучение модели может проводиться с её учётом, чтобы минимизировать потерю качества.

## [9. Дропаут.](#Машинное-обучение-2025)

Согласно **Лекции 2**, **Дропаут (Dropout)** — это техника регуляризации, которая заключается в случайном "отключении" (обнулении выходов) части нейронов на каждой итерации обучения. Это делается с некоторой вероятностью, обычно 0.5. В результате на каждом шаге обучения используется немного разная "мини-сеть", но все они разделяют общие параметры.

**Цели и эффекты дропаута:**

*   **Борьба с переобучением:** Это основная цель. Дропаут не позволяет нейронам "привыкать" друг к другу (снижает **коадаптацию**) и заставляет сеть изучать более устойчивые и **робастные** признаки.
*   **Недостаток:** Как отмечается в лекции, применение дропаута примерно вдвое увеличивает количество итераций, необходимых для сходимости модели.

Помимо своей основной роли в регуляризации во время обучения, в **Лекции 14** упоминается и другое применение дропаута — для **оценки неопределённости (дисперсии)** предсказаний модели на этапе инференса (тестирования). Для этого модель запускается несколько раз с активным дропаутом, и разброс полученных предсказаний используется как мера уверенности модели в своём ответе.

## [10. Пакетная нормализация.](#Машинное-обучение-2025)

Согласно **Лекции 2**, **Пакетная нормализация (Batch Normalization)** — это техника, предназначенная для ускорения и стабилизации обучения глубоких нейронных сетей. Она решает проблему изменения распределения выходов каждого слоя в процессе обучения (известную как **внутренний ковариантный сдвиг**), поддерживая постоянство среднего и дисперсии входных данных для следующего слоя.

Процесс Batch Normalization состоит из двух шагов и применяется к каждому пакету (батчу) данных:

1.  **Нормализация:** Для каждого признака (фичи) внутри батча вычисляются его среднее значение (`E[x_d]`) и дисперсия (`D[x_d]`). Затем каждый признак стандартизируется:
    $\hat{x}_d = \frac{x_d - E[x_d]}{\sqrt{D[x_d] + \epsilon}}$
    Здесь `ε` — это малая добавка для численной стабильности.

2.  **Масштабирование и сдвиг:** Чтобы сеть не теряла в выразительности и могла при необходимости "отменить" нормализацию, к стандартизированному значению применяется линейное преобразование с обучаемыми параметрами:
    $y_d = \gamma \cdot \hat{x}_d + \beta$
    Здесь **`γ` (гамма)** и **`β` (бета)** — это обучаемые параметры масштаба и сдвига соответственно, которые обновляются в процессе градиентного спуска, как и любые другие веса сети.

Как отмечено в лекции, Batch Normalization значительно **ускоряет сходимость** модели и часто выступает в роли регуляризатора, делая другие методы, такие как Дропаут, менее необходимыми.

## [11. Инициализация параметров методы Xavier и He.](#Машинное-обучение-2025)

Согласно **Лекции 2**, правильная инициализация параметров является критически важным шагом для успешного обучения глубоких нейронных сетей. Она помогает бороться с **проблемой затухания и взрыва градиентов**, о которой упоминается в **Лекции 1**. Неправильная инициализация (например, нулями или одинаковыми значениями для весов) приведёт к тому, что все нейроны в слое будут обучаться одинаково.

В конспекте приводятся следующие общие правила:
*   **Векторы сдвига (bias):** Можно инициализировать нулями.
*   **Весовые матрицы:** Необходимо инициализировать случайными значениями.

Для весовых матриц рассматриваются два ключевых метода: **Xavier** и **He**.

### Метод Xavier (или Glorot)

*   **Назначение:** Этот метод подходит для функций активации, которые симметричны и приблизительно линейны вблизи нуля, таких как **гиперболический тангенс (tanh)**.
*   **Идея:** Подобрать дисперсию случайных весов таким образом, чтобы дисперсия активаций и градиентов оставалась примерно постоянной при прохождении через слои сети, предотвращая их затухание или взрыв.
*   **Формула:** На практике веса инициализируются из равномерного распределения (`U`):
    $w_d \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_d + n_{d+1}}}, \frac{\sqrt{6}}{\sqrt{n_d + n_{d+1}}}\right]$
    где `n_d` — число нейронов на входе слоя, а `n_{d+1}` — число нейронов на выходе.

### Метод He

*   **Назначение:** Метод был разработан специально для слоёв с функцией активации **ReLU**. Так как ReLU обнуляет все отрицательные значения, дисперсия её выходов вдвое меньше, что требует коррекции в инициализации.
*   **Идея:** Компенсировать "потерю" половины активаций из-за ReLU, чтобы поддерживать стабильную дисперсию.
*   **Формула:** Веса инициализируются из нормального распределения (`N`) с нулевым средним:
    $w_d \sim N\left(0, \frac{2}{n_d}\right)$
    где `n_d` — число нейронов на входе слоя.

### Сравнительная таблица

| Метод | Предназначен для активации | Формула инициализации (из Лекции 2) |
| :--- | :--- | :--- |
| **Xavier** | `tanh` и другие симметричные | Равномерное распределение, $D(w_d) = \frac{2}{n_d + n_{d+1}}$ |
| **He** | `ReLU` и её вариации | Нормальное распределение, $D(w_d) = \frac{2}{n_d}$ |

Таким образом, выбор метода инициализации напрямую зависит от используемой в слое функции активации, что является ключевым для стабильного обучения глубоких сетей.

## [12. Статический и динамический граф вычислений.](#Машинное-обучение-2025)

Согласно **Лекции 3**, для автоматического дифференцирования любая сложная функция представляется в виде **вычислительного графа**. В узлах этого графа находятся операции (например, сложение, умножение, синус), а рёбрами передаются данные. Конспект выделяет два подхода к построению таких графов: статический и динамический.

### Статический граф вычислений

*   **Определение:** Статический граф **строится заранее**, до начала реальных вычислений. Архитектура сети полностью определяется и компилируется один раз.
*   **Преимущества:** Поскольку вся структура графа известна заранее, её можно **оптимизировать**: объединять операции, эффективно распределять память и распараллеливать вычисления. Это может привести к более высокой производительности.
*   **Недостатки:** Меньшая гибкость. Архитектура не может изменяться в зависимости от входных данных, что усложняет работу с данными переменной длины (например, в RNN). Отладка также может быть сложнее.

### Динамический граф вычислений

*   **Определение:** Динамический граф **формируется на лету**, в процессе выполнения кода. Каждая операция сразу же выполняется и добавляется в граф.
*   **Преимущества:** Архитектура **более гибкая** и может зависеть от входных данных (например, количество шагов рекуррентной сети может соответствовать длине предложения). Процесс отладки значительно проще, так как он похож на отладку обычного императивного кода.
*   **Недостатки:** Глобальные оптимизации сложнее, так как полная структура графа неизвестна заранее.

### Сравнительная таблица

| Характеристика | Статический граф | Динамический граф |
| :--- | :--- | :--- |
| **Когда строится?** | **Заранее**, до вычислений. | **На лету**, в процессе вычислений. |
| **Гибкость** | Низкая. Архитектура фиксирована. | Высокая. Архитектура может меняться. |
| **Оптимизация** | Высокая, так как весь граф известен. | Сложнее, оптимизируется по частям. |
| **Отладка** | Сложнее. | Проще. |

В лекции не указано, какие фреймворки используют тот или иной подход, но, опираясь на общие знания, можно отметить, что TensorFlow 1.x был ярким представителем статических графов, а PyTorch и TensorFlow 2.x популяризовали динамический подход.

## [13. Прямое и обратное дифференцирование.](#Машинное-обучение-2025)

Согласно **Лекции 3**, для вычисления градиентов функции ошибки в машинном обучении используется **автоматическое дифференцирование**, которое выполняется на **вычислительном графе**. Существует два основных подхода к этому процессу: прямое и обратное дифференцирование.

### Прямое дифференцирование (Forward Mode)

При прямом дифференцировании вычисление производных происходит **в том же направлении, что и вычисление самой функции** — от входов к выходу. На каждом шаге вместе со значением функции вычисляется и её производная по входным переменным. Этот метод эффективен, когда у нас мало входов и много выходов. Однако в задачах машинного обучения ситуация обратная: у нас миллионы входных параметров (веса модели) и всего один выход (функция ошибки). Как следствие, для получения градиента по всем параметрам потребовалось бы выполнить по одному проходу для каждого параметра, что делает этот метод **неэффективным** для обучения нейронных сетей.

### Обратное дифференцирование (Backward Mode)

Обратное дифференцирование, напротив, выполняет вычисления **в обратном порядке** — от конечного выхода (функции ошибки) к входам (параметрам модели). Этот метод основан на **цепном правиле (chain rule)** и позволяет за один-единственный проход по графу в обратном направлении вычислить градиент функции ошибки по всем параметрам модели одновременно. Именно благодаря своей высокой эффективности по времени и памяти этот способ стал стандартом де-факто и используется во всех современных фреймворках для глубокого обучения. В **Лекции 3** также уточняется, что исторически сложившийся термин **"обратное распространение ошибки" (backpropagation)** по сути и является реализацией обратного автоматического дифференцирования.

### Сравнительная таблица

| Характеристика | Прямое дифференцирование (Forward Mode) | Обратное дифференцирование (Backward Mode) |
| :--- | :--- | :--- |
| **Направление обхода графа** | От входов к выходу (вперёд) | От выхода к входам (назад) |
| **Что вычисляется?** | Производные выхода по **одному** входу | Производные **одного** выхода по **всем** входам |
| **Эффективность для ML** | **Низкая**. Требуется проход для каждого параметра. | **Высокая**. Все градиенты вычисляются за один проход. |
| **Основное применение** | В задачах, где число входов меньше числа выходов. | **Стандарт для обучения нейронных сетей**. |

## [14. Импульсный и адаптивный градиентный спуск.](#Машинное-обучение-2025)

Согласно **Лекции 3**, стандартный градиентный спуск имеет недостатки, такие как медленная сходимость в "долинах" функции потерь (эффект зигзага). Для решения этих проблем были разработаны более продвинутые методы оптимизации, которые можно разделить на импульсные и адаптивные.

### Импульсные методы (Momentum-based)

Эти методы добавляют "инерцию" в процесс обновления весов, сглаживая траекторию движения и ускоряя сходимость.

*   **Momentum (Импульс):** Этот метод накапливает экспоненциально затухающее среднее прошлых градиентов и движется в этом направлении. Это помогает "проскакивать" мелкие локальные минимумы и двигаться быстрее вдоль длинных, узких "оврагов" функции потерь, устраняя зигзагообразное движение.

*   **Nesterov Momentum:** Это улучшение классического Momentum. Вместо того чтобы вычислять градиент в текущей точке и затем делать шаг в направлении накопленного импульса, метод Nesterov сначала делает "предварительный" шаг в направлении импульса, а затем вычисляет градиент в этой новой точке и использует его для коррекции направления. Как указано в лекции, это обеспечивает теоретически лучшую сходимость для выпуклых задач.

### Адаптивные методы

Эти методы адаптируют скорость обучения (learning rate) для каждого параметра модели индивидуально. Параметры, которые обновляются редко, получают большие шаги, а часто обновляемые — меньшие.

*   **Adagrad (Adaptive Gradient):** Накапливает сумму квадратов прошлых градиентов для каждого параметра. Чем больше накопленный градиент, тем меньше становится шаг. **Проблема:** со временем шаги могут стать слишком маленькими, и обучение практически останавливается ("затухает").

*   **RMSProp (Root Mean Square Propagation):** Решает проблему затухания Adagrad. Вместо накопления всей истории квадратов градиентов, RMSProp использует их экспоненциально затухающее среднее. Это позволяет "забывать" старые градиенты и поддерживать адекватную скорость обучения.

*   **Adadelta:** Является модификацией RMSProp, которая, согласно лекции, устраняет необходимость в ручной установке глобальной скорости обучения.

### Гибридный метод

*   **Adam (Adaptive Moment Estimation):** Это самый популярный на сегодняшний день оптимизатор. Он **совмещает идеи Momentum и RMSProp**. Для каждого параметра Adam хранит экспоненциально затухающие средние как первого момента градиентов (аналогично Momentum), так и второго момента (аналогично RMSProp). Он также применяет специальную коррекцию смещения (bias correction) на начальных этапах обучения.

### Сравнительная таблица (по данным Лекции 3)

| Метод | Основная идея | Ключевая особенность / Проблема |
| :--- | :--- | :--- |
| **Momentum** | Добавляет "инерцию" к шагу градиента. | Ускоряет движение вдоль "оврагов", сглаживает зигзаги. |
| **Nesterov** | Корректирует импульс, "заглядывая вперёд". | Более быстрая сходимость на выпуклых задачах. |
| **Adagrad** | Адаптивный шаг на основе всей истории градиентов. | **Проблема:** Шаг со временем затухает. |
| **RMSProp** | Адаптивный шаг на основе скользящего среднего. | Решает проблему затухания Adagrad. |
| **Adadelta** | Улучшение RMSProp. | Не требует ручной установки скорости обучения. |
| **Adam** | Комбинация идей Momentum и RMSProp. | Эффективен, является стандартным выбором во многих задачах. |

## [15. Двумерная свёртка. Пулинг.](#Машинное-обучение-2025)

Согласно **Лекции 5**, двумерная свёртка и пулинг являются ключевыми операциями в **свёрточных нейронных сетях (CNN)**, которые лежат в основе современного компьютерного зрения.

### Двумерная свёртка (2D Convolution)

**Свёртка** — это основная операция для извлечения признаков из изображений. Она основана на трёх концепциях: **локальное восприятие**, **общие параметры** и **ядра (фильтры)**. Вместо того чтобы обрабатывать всё изображение целиком, свёртка применяет небольшую матрицу, называемую **ядром** (или фильтром), к локальным участкам изображения. Это ядро "скользит" по всему изображению, вычисляя в каждой позиции скалярное произведение между своими значениями и значениями пикселей под ним. Результаты этих вычислений формируют новую матрицу, называемую **картой признаков (feature map)**. Поскольку одно и то же ядро используется для всего изображения, параметры являются общими, что резко сокращает количество обучаемых весов.

На размер выходной карты признаков влияют три параметра:
*   **Размер ядра (Kernel Size, K):** Размер фильтра (например, 3x3, 5x5).
*   **Шаг (Stride, S):** Шаг, с которым ядро перемещается по изображению.
*   **Дополнение (Padding, P):** Количество пикселей (обычно нулей), добавляемых по краям изображения для контроля размера выхода.

Формула для расчёта размера выхода, приведённая в конспекте:
$O = \frac{I - K + 2P}{S} + 1$
где `O` — размер выхода, `I` — размер входа.

### Пулинг (Pooling)

**Пулинг** (также **субдискретизация** или **subsampling**) — это операция, которая обычно следует за свёрткой и используется для уменьшения пространственной размерности карт признаков. Это помогает сделать представление более компактным и инвариантным к небольшим сдвигам и масштабированию объекта на изображении. Как и свёртка, пулинг работает с небольшим окном, которое скользит по карте признаков, но вместо скалярного произведения он применяет простую агрегирующую функцию.

В лекции упоминаются два основных вида пулинга:
*   **MaxPooling:** Из каждого окна выбирается максимальное значение.
*   **AveragePooling:** Вычисляется среднее значение всех элементов в окне.

Размер выхода после пулинга рассчитывается по аналогичной формуле, где `K` — это размер окна пулинга. Вместе слои свёртки и пулинга формируют иерархическую структуру, которая на нижних уровнях выделяет простые признаки (края, углы), а на верхних — более сложные (текстуры, части объектов).

## [16. Семантическая сегментация. Локализация. Детекция. Сегментация объектов.](#Машинное-обучение-2025)

Согласно **Лекции 5**, локализация, детекция и семантическая сегментация — это фундаментальные задачи компьютерного зрения с разной степенью сложности, которые решаются с помощью свёрточных нейронных сетей.

*   **Локализация объектов (Object Localization):** Это задача определения местоположения **одного** объекта на изображении. Результатом является класс объекта и координаты ограничивающей его рамки (bounding box). В конспекте она описана как "определение координат объектов внутри изображения".

*   **Распознавание объектов (Object Detection):** Это более сложная задача, расширяющая локализацию на **несколько** объектов. Модель должна найти все интересующие объекты на изображении, нарисовать вокруг каждого из них рамку и определить класс каждого объекта. В лекции упоминаются ключевые подходы для решения этой задачи, такие как **R-CNN** и **YOLO (You Only Look Once)**, а также датасет **Pascal VOC**.

*   **Семантическая сегментация (Semantic Segmentation):** Это наиболее детальная задача из перечисленных. Её цель — классифицировать **каждый пиксель** на изображении, присвоив ему метку определённого класса (например, "дорога", "машина", "небо"). Результатом является не рамка, а "маска" или карта сегментации, где каждый цвет соответствует своему классу. Для решения этой задачи используются специальные архитектуры, такие как **U-Net**, и техники для повышения разрешения карты признаков, например, **деконволюция** или **транспонированная свёртка (Transposed Convolution)**.

В конспектах термин "сегментация объектов" отдельно не определяется. Однако задача **семантической сегментации** подробно описана и, вероятно, является тем, что имеется в виду.

### Сравнительная таблица задач

| Задача | Основная цель | Выход модели | Примеры подходов (из Лекции 5) |
| :--- | :--- | :--- | :--- |
| **Локализация** | Найти и ограничить **один** объект | Класс + 1 рамка | *(не указаны)* |
| **Детекция** | Найти и ограничить **все** объекты | Набор пар (класс + рамка) | R-CNN, YOLO |
| **Семантическая сегментация**| Классифицировать **каждый пиксель**| Карта классов (маска) для всего изображения | U-Net, Деконволюция |

## [17. UNET. R-CNN. YOLO.](#Машинное-обучение-2025)

Согласно **Лекции 5**, U-Net, R-CNN и YOLO — это ключевые архитектуры и подходы, используемые для решения различных задач компьютерного зрения, таких как семантическая сегментация и распознавание объектов.

### U-Net

*   **Задача:** **Семантическая сегментация**.
*   **Идея:** U-Net — это **"типичная архитектура для сегментации"**. Её цель — классифицировать каждый пиксель изображения. Для этого в таких архитектурах используются специальные слои для повышения размерности карт признаков (апсемплинга), такие как **деконволюция** или **транспонированная свёртка (Transposed Convolution)**, чтобы итоговая карта классов (маска) соответствовала размеру исходного изображения.

### R-CNN (Regions with CNN features)

*   **Задача:** **Распознавание объектов (Object Detection)**.
*   **Идея:** R-CNN — это многоэтапный подход. Его принцип работы, описанный в лекции как **"рекурсивные регионы интереса и классификация этих прямоугольников"**, заключается в следующем:
    1.  Сначала на изображении выделяются потенциальные "регионы интереса" — области, где с высокой вероятностью могут находиться объекты.
    2.  Затем каждая из этих областей (прямоугольников) по отдельности подаётся на вход свёрточной сети для классификации и уточнения её границ.

### YOLO (You Only Look Once)

*   **Задача:** **Распознавание объектов (Object Detection)**.
*   **Идея:** В отличие от многоэтапного R-CNN, YOLO позиционируется как **"быстрый end-to-end подход"**. Это означает, что модель обрабатывает всё изображение за один проход, одновременно предсказывая и ограничивающие рамки, и классы для всех объектов. Такая архитектура значительно быстрее, что и отражено в названии "You Only Look Once" (Ты смотришь только один раз).

### Сводная таблица

| Модель / Подход | Основная задача | Ключевая идея (согласно Лекции 5) |
| :--- | :--- | :--- |
| **U-Net** | Семантическая сегментация | Классификация каждого пикселя с использованием апсемплинга (деконволюции) для восстановления размера. |
| **R-CNN** | Распознавание объектов | Двухэтапный процесс: сначала поиск регионов, затем их классификация. |
| **YOLO** | Распознавание объектов | Быстрый, однопроходный (end-to-end) процесс: одновременное предсказание рамок и классов. |

## [18. Рекуррентные сети.](#Машинное-обучение-2025)

Согласно **Лекции 7**, **рекуррентные нейронные сети (RNN)** — это класс моделей, созданный для работы с **последовательностями** данных, такими как временные ряды, текст или речь. В отличие от полносвязных и свёрточных сетей, RNN имеют внутреннюю "память", которая позволяет им обрабатывать информацию с учётом предыдущего контекста.

### Базовое устройство

Основной принцип RNN заключается в том, что для каждого элемента последовательности $t$ вычисление зависит не только от текущего входа $x_t$, но и от **скрытого состояния** $h_{t-1}$ из предыдущего шага. Это скрытое состояние $h_t$ служит "памятью" сети. Процесс можно описать так:
$h_t = f_a(x_t, h_{t-1})$
$y_t = f_b(h_t)$
где $f_a$ и $f_b$ — это обучаемые функции (например, полносвязные слои). Биологической мотивацией для RNN служат реальные нейронные сети, которые по своей природе рекуррентны.

### Проблемы и решения

Классические RNN страдают от серьёзных недостатков:
*   **Проблема затухающего/взрывающегося градиента:** В длинных последовательностях градиенты либо исчезают, либо становятся слишком большими, что мешает обучению долгосрочных зависимостей.
*   **Краткосрочная память:** Старое скрытое состояние постоянно перезаписывается, что мешает хранить информацию надолго.

Для решения этих проблем были разработаны более сложные рекуррентные ячейки:

1.  **LSTM (Long Short-Term Memory — Долгая краткосрочная память):**
    Ключевая идея LSTM — это введение отдельной **"ленты памяти" (cell state)**, которая позволяет информации течь через последовательность почти без изменений. Управление этой памятью осуществляется с помощью специальных "вентилей" (gates):
    *   **Фильтр забывания (Forget Gate):** Решает, какую информацию из старой памяти нужно удалить.
    *   **Фильтр входа (Input Gate):** Определяет, какая новая информация будет сохранена.
    *   **Фильтр выхода (Output Gate):** Контролирует, какая часть памяти будет использована для вычисления скрытого состояния на текущем шаге.

2.  **GRU (Gated Recurrent Unit):**
    Это **"облегчённый аналог LSTM"**, который объединяет некоторые вентили и имеет меньше параметров. GRU также эффективно борется с проблемой затухающего градиента, но является вычислительно более простой.

### Продвинутые архитектуры

В **Лекции 7** также описываются более сложные архитектуры на основе RNN:

*   **Bidirectional RNN (Двунаправленная RNN):** Состоит из двух RNN, обрабатывающих последовательность в противоположных направлениях (слева направо и справа налево). Их выходы объединяются, что позволяет модели учитывать как прошлый, так и будущий контекст для каждого элемента.

*   **Seq2Seq (Sequence-to-sequence):** Архитектура из двух частей — **энкодера** и **декодера**. Энкодер "сжимает" входную последовательность в один вектор состояния (контекст-вектор), а декодер разворачивает этот вектор в выходную последовательность. Используется, например, в машинном переводе. Усовершенствованием этой архитектуры является **механизм внимания (attention)**, который позволяет декодеру на каждом шаге "смотреть" на все скрытые состояния энкодера, а не только на последний.

### RNN в сравнении с Трансформерами

Как отмечается в **Лекции 8**, главным недостатком RNN является их **последовательная природа вычислений**, которая не позволяет эффективно распараллеливать обработку длинных последовательностей. Трансформеры, отказавшись от рекурсии в пользу механизма внимания, решили эту проблему, что и привело к их доминированию в современных задачах обработки естественного языка.

## [19. LSTM.](#Машинное-обучение-2025)

Согласно **Лекции 7**, **LSTM (Long Short-Term Memory — Долгая краткосрочная память)** — это продвинутая рекуррентная ячейка, разработанная для решения ключевых проблем классических RNN. В частности, она эффективно борется с **проблемой затухания/взрыва градиентов** и позволяет модели **хранить информацию в течение длительного времени**.

Основная идея LSTM — введение отдельного состояния, называемого **"лентой памяти" (cell state, `C_t`)**, которое передаётся от шага к шагу. Поток информации в этой памяти и из неё контролируется тремя специальными механизмами — **вентилями (gates)**.

### Устройство LSTM-ячейки

Каждый вентиль представляет собой полносвязный слой с сигмоидной функцией активации, выход которого (число от 0 до 1) определяет, какая доля информации пройдёт через него.

1.  **Фильтр забывания (Forget Gate):**
    *   **Назначение:** Решает, какую часть информации из предыдущего состояния памяти (`C_{t-1}`) необходимо "забыть".
    *   **Формула:** $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

2.  **Фильтр входа (Input Gate):**
    *   **Назначение:** Определяет, какая новая информация будет сохранена в памяти. Он состоит из двух частей:
        *   Определение того, *что* обновлять (`i_t`).
        *   Создание вектора-кандидата с новой информацией (`\tilde{C}_t`).
    *   **Формулы:**
        $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
        $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$

3.  **Обновление памяти:**
    *   Старая память (`C_{t-1}`) умножается на результат фильтра забывания (`f_t`), а новая информация-кандидат (`\tilde{C}_t`) — на результат фильтра входа (`i_t`).
    *   **Формула:** $C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$

4.  **Фильтр выхода (Output Gate):**
    *   **Назначение:** Определяет, какая часть обновлённой памяти (`C_t`) пойдёт в скрытое состояние на выходе (`h_t`).
    *   **Формулы:**
        $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
        $h_t = o_t \cdot \tanh(C_t)$

### Анализ LSTM

В лекции приводится следующий анализ:

| Достоинства | Недостатки |
| :--- | :--- |
| Хорошо работает с длинными зависимостями. | Сложное и долгое обучение. |
| Практически нет проблем с градиентами. | Может забывать очень длинные зависимости. |

В качестве более простого аналога LSTM в конспекте упоминается **GRU (Gated Recurrent Unit)**, который имеет меньше параметров за счёт слияния некоторых операций, но следует тому же принципу управления потоками информации.

## [20. Трансформеры.](#Машинное-обучение-2025)

Согласно вашим конспектам, **Трансформеры** — это современный класс архитектур нейронных сетей, который стал стандартом де-факто для обработки последовательностей, в первую очередь в задачах обработки естественного языка (NLP). Их ключевое отличие и преимущество перед рекуррентными сетями (RNN) заключается в **полном отказе от рекурсии** в пользу **механизма внимания**, что позволяет эффективно распараллеливать вычисления.

Информация о Трансформерах содержится преимущественно в **Лекции 8**, а также упоминается в **Лекциях 7 и 10**.

### 1. Механизм Внимания (Attention Mechanism)

Внимание — это ядро трансформера. Вместо последовательной обработки, как в RNN, механизм внимания позволяет каждому элементу последовательности напрямую "взаимодействовать" со всеми остальными элементами, взвешивая их важность.

*   **Scaled Dot-Product Attention:** Это конкретная реализация внимания, используемая в трансформерах. Для каждого входного вектора `X` создаются три его проекции с помощью обучаемых матриц:
    *   **Query (Запрос, Q):** `Q = XW_Q` — что я ищу?
    *   **Key (Ключ, K):** `K = XW_K` — что я могу предложить?
    *   **Value (Значение, V):** `V = XW_V` — что я на самом деле из себя представляю?

    Внимание вычисляется по формуле:
    $Z = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \cdot V$
    Здесь `d_k` — это размерность векторов `Q` и `K`. Деление на `sqrt(d_k)`, как указано в лекции, необходимо для стабилизации градиентов.

*   **Multi-Head Attention (Многоголовое внимание):** Вместо одного механизма внимания используется несколько ("головы"), работающих параллельно. Каждая "голова" обучается своим матрицам проекций и может фокусироваться на разных аспектах взаимосвязей в данных. Результаты всех голов затем конкатенируются и снова линейно преобразуются.

### 2. Архитектура Трансформера

Классический трансформер состоит из двух основных частей: **кодировщика (Encoder)** и **декодера (Decoder)**, каждый из которых представляет собой стопку одинаковых блоков. Каждый блок, в свою очередь, состоит из слоя Multi-Head Attention и простого полносвязного слоя (Feed-Forward Network). Вокруг этих слоёв используются остаточные соединения и нормализация.

На основе этой архитектуры были созданы две основные ветви моделей:
*   **BERT (Encoder-only):** Использует **двунаправленное внимание**, то есть каждый токен "смотрит" на все остальные токены в последовательности. Это идеально подходит для задач **понимания** языка (классификация, извлечение сущностей).
*   **GPT (Decoder-only):** Использует **авторегрессионное внимание**, где каждый токен может "смотреть" только на предыдущие. Это достигается с помощью **маскирования** (замены значений в матрице внимания на `-∞`), что делает такие модели идеальными для **генерации** текста.

### 3. Позиционное кодирование (Positional Encoding)

Поскольку механизм внимания сам по себе не учитывает порядок элементов в последовательности (он работает с ними как с **множеством**, о чём говорится в **Лекции 10**), в трансформеры необходимо вручную добавлять информацию о позиции.

*   **Классический подход:** К входным эмбеддингам прибавляются специальные векторы, значения которых вычисляются на основе синусов и косинусов разной частоты.
    $PE(t, 2i) = \sin(t / 10000^{2i/d})$
    $PE(t, 2i+1) = \cos(t / 10000^{2i/d})$
    где `t` — позиция, `i` — размерность.

*   **Современные подходы:** В лекции также упоминаются более продвинутые методы, такие как **RoPE**, **ALiBi** и **обучаемое позиционное кодирование**.

### 4. Преимущества и недостатки Трансформеров

| Преимущества (из Лекции 8) | Недостатки (из Лекции 8) |
| :--- | :--- |
| **Распараллеливаемость:** Отсутствие рекурсии позволяет обрабатывать все токены одновременно. | **Очень много параметров:** Модели могут достигать сотен миллиардов параметров. |
| **State-of-the-art качество:** Показывают лучшие результаты на многих задачах. | **Нестабильное обучение:** Требуют специальных техник для стабилизации. |
| **Интерпретируемость:** Можно анализировать матрицы внимания, чтобы понять, "куда смотрит" модель. | **Ограничение по длине последовательности:** Вычислительная сложность растёт квадратично с длиной входа. |

### 5. Применение

Трансформеры являются основой современных **Больших Языковых Моделей (LLM)**. В **Лекции 8** упоминаются такие продвинутые техники, как **RAG (Retrieval Augmented Generation)** и **Chain-of-Thought**, которые используются для повышения их производительности.

## [21. Позиционное кодирование.](#Машинное-обучение-2025)

Согласно **Лекции 8**, **позиционное кодирование (Positional Encoding)** — это механизм, который добавляет информацию о порядке или положении элементов в последовательности. Это необходимо, поскольку **механизм внимания**, лежащий в основе трансформеров, по своей природе является перестановочно-инвариантным, то есть он обрабатывает входные данные как неупорядоченное множество (**Лекция 10**). Без позиционного кодирования трансформер не смог бы различить предложения "собака укусила человека" и "человека укусила собака".

В конспекте описаны следующие методы позиционного кодирования:

### 1. Классическое синусоидальное кодирование

*   **Идея:** Для каждой позиции в последовательности генерируется уникальный вектор (позиционный эмбеддинг), который затем **прибавляется** к эмбеддингу соответствующего токена. Этот вектор не обучаемый, а вычисляется по детерминированным формулам с использованием синусов и косинусов разной частоты.
*   **Формулы:** Для позиции `t` и размерности `i`:
    $PE(t, 2i) = \sin(t / 10000^{2i/d})$
    $PE(t, 2i+1) = \cos(t / 10000^{2i/d})$
    где `d` — это общая размерность эмбеддинга. Такая схема позволяет модели легко определять относительные позиции, так как для любого смещения существует линейное преобразование, связывающее позиционные эмбеддинги.

### 2. Вариации и современные подходы

В **Лекции 8** также перечислены более продвинутые методы:

*   **Обучаемое позиционное кодирование:** Вместо использования фиксированных синусоид, для каждой позиции создаётся свой собственный **обучаемый вектор-эмбеддинг**. Недостаток этого метода в том, что он плохо обобщается на последовательности длиннее, чем те, что были в обучающей выборке, и теряет информацию об относительном порядке.

*   **RoPE (Rotary Positional Embedding):** Вместо прибавления позиционного вектора, RoPE **поворачивает** векторы `Query` и `Key` на угол, пропорциональный их позиции. Это элегантный способ внедрения информации об абсолютной и относительной позиции непосредственно в механизм внимания.

*   **ALiBi (Attention with Linear Bias):** Этот метод не модифицирует входные эмбеддинги, а добавляет **линейное смещение (bias)** прямо в матрицу внимания до применения `softmax`. Величина смещения зависит от расстояния между токенами `Query` и `Key`. Это позволяет модели естественным образом придавать больший вес близким токенам.

Таким образом, позиционное кодирование — это критически важный компонент, обеспечивающий чувствительность трансформеров к порядку слов в предложении.

## [22. Частота Найквиста.](#Машинное-обучение-2025)

Вы правы, в предоставленных конспектах лекций понятие **частоты Найквиста** не упоминается. Привожу объяснение на основе общих знаний в области обработки сигналов.

---

**Теорема Котельникова (или теорема Найквиста-Шеннона)** — это фундаментальный принцип в цифровой обработке сигналов. Он гласит, что для точного восстановления аналогового (непрерывного) сигнала из его цифровых (дискретных) отсчётов, частота дискретизации $f_s$ должна быть как минимум в два раза выше максимальной частоты $f_{max}$, присутствующей в исходном сигнале.

$f_s \geq 2 \cdot f_{max}$

**Частота Найквиста** — это и есть та самая минимально допустимая частота дискретизации, равная удвоенной максимальной частоте сигнала:

$\text{Частота Найквиста} = 2 \cdot f_{max}$

### Что происходит, если нарушить это правило?

Если частота дискретизации ниже частоты Найквиста ($f_s < 2 \cdot f_{max}$), возникает явление, называемое **алиасинг (aliasing)** или **наложение спектров**. При этом высокочастотные компоненты сигнала "маскируются" под низкочастотные, что приводит к необратимому искажению информации. Сигнал уже невозможно будет восстановить в его исходном виде.

Простой пример: если смотреть на быстро вращающиеся лопасти вертолёта или спицы колеса автомобиля, иногда кажется, что они вращаются медленно или даже в обратную сторону. Это визуальный пример алиасинга, где частота "дискретизации" (частота кадров камеры или стробоскопа) недостаточно высока, чтобы правильно зафиксировать быстрое движение.

В машинном обучении теорема Котельникова важна при работе с временными рядами, аудиосигналами, радиосигналами и любыми другими данными, которые изначально являются аналоговыми и требуют оцифровки перед обработкой. Правильный выбор частоты дискретизации критичен для сохранения всей значимой информации в сигнале.

## [23. Преобразование Фурье.](#Машинное-обучение-2025)

В предоставленных конспектах прямого и детального разбора **преобразования Фурье** нет. Однако в **Лекции 7 "Рекуррентные сети и работа с последовательностями"** упоминается **спектральный анализ** как один из классических методов работы с числовыми последовательностями, а преобразование Фурье является его основой.

Отвечаю на основе общих знаний.

---

**Преобразование Фурье** — это математический инструмент, который позволяет разложить сложный сигнал (например, временной ряд или звуковую волну) на составляющие его простые синусоидальные и косинусоидальные волны разной частоты и амплитуды. Иными словами, оно переводит представление сигнала из **временной области** (как сигнал меняется во времени) в **частотную область** (какие частоты и с какой интенсивностью присутствуют в сигнале).

### Ключевая идея

Любой сложный периодический сигнал можно представить как сумму простых синусов и косинусов. Преобразование Фурье находит, какие именно синусы и косинусы (с какими частотами, амплитудами и фазами) нужно сложить, чтобы получить исходный сигнал. Результатом преобразования является **спектр** сигнала.

### Для чего это нужно?

1.  **Анализ частот:** Понять, из каких частотных компонент состоит сигнал. Например, в звуке это позволяет выделить основной тон и обертоны, определяющие тембр инструмента. В анализе временных рядов это помогает найти скрытые периодичности и циклы (например, сезонность в продажах).
2.  **Фильтрация:** В частотной области очень легко удалять нежелательные компоненты. Например, можно убрать высокочастотный шум (фильтр низких частот) или низкочастотный гул (фильтр высоких частот), просто "обнулив" соответствующие частоты в спектре и выполнив обратное преобразование Фурье.
3.  **Сжатие данных:** Многие сигналы (например, звук или изображения) имеют мало значащих высокочастотных компонент. Их можно отбросить с минимальной потерей качества, что используется в форматах сжатия, таких как JPEG и MP3.

### Дискретное и быстрое преобразование Фурье

В цифровой обработке сигналов используется **Дискретное преобразование Фурье (ДПФ, DFT)**, которое работает с дискретными (цифровыми) сигналами. Для его эффективного вычисления был разработан **алгоритм быстрого преобразования Фурье (БПФ, FFT)**, который значительно сокращает количество вычислений и является одним из самых важных алгоритмов в цифровой обработке сигналов и научных вычислениях.

В машинном обучении преобразование Фурье используется для извлечения признаков из временных рядов, аудиоданных и изображений, а также в некоторых архитектурах нейронных сетей для эффективного выполнения операций, похожих на свёртку.

## [24. Спектрограммы Мела. MFCC.](#Машинное-обучение-2025)

В предоставленных конспектах **спектрограммы Мела** и **MFCC** не упоминаются. Информация ниже основана на общих знаниях в области обработки аудиосигналов и машинного обучения.

---

Спектрограммы Мела и MFCC (Mel-Frequency Cepstral Coefficients) — это два стандартных и очень популярных способа представления аудиосигнала в виде признаков для моделей машинного обучения, особенно в задачах распознавания речи, классификации звуков и музыки. Оба метода основаны на особенностях человеческого слуха.

### Шкала Мел (Mel Scale)

Человеческий слух воспринимает высоту звука нелинейно. Мы хорошо различаем изменения низких частот (например, разницу между 100 Гц и 200 Гц), но гораздо хуже — изменения высоких (разница между 10 000 Гц и 10 100 Гц для нас почти незаметна). **Шкала Мел** — это психоакустическая шкала высоты звука, которая имитирует эту особенность: она почти линейна на низких частотах и логарифмична на высоких.

### 1. Спектрограммы Мела (Mel Spectrograms)

Это модифицированная версия обычной спектрограммы (которая показывает, как спектр частот сигнала меняется во времени), где ось частот преобразована в шкалу Мел.

Процесс получения Мел-спектрограммы:
1.  **Краткосрочное преобразование Фурье (STFT):** Исходный аудиосигнал разбивается на короткие, перекрывающиеся временные окна, и для каждого окна вычисляется его спектр с помощью Быстрого преобразования Фурье (FFT). Результатом является обычная спектрограмма.
2.  **Применение Мел-фильтров:** Полученный спектр пропускается через набор треугольных фильтров, расположенных равномерно по шкале Мел. Каждый фильтр агрегирует энергию сигнала в определённом частотном диапазоне Мел-шкалы.
3.  **Логарифмирование:** Энергия на выходе каждого фильтра логарифмируется, так как человеческий слух воспринимает громкость логарифмически.

Результатом является изображение (**Мел-спектрограмма**), где по оси X — время, по оси Y — частоты в Мел-шкале, а цвет/яркость пикселя — энергия (громкость) на данной частоте в данный момент времени. Эти "картинки" очень хорошо подходят для обработки **свёрточными нейронными сетями (CNN)**.

### 2. Кепстральные коэффициенты на частотах Мел (MFCC)

MFCC — это дальнейшее сжатие и декорреляция информации из Мел-спектрограммы. Они представляют собой компактный набор признаков, который ещё лучше отражает тембральные характеристики звука.

Процесс получения MFCC (продолжение процесса для Мел-спектрограммы):
4.  **Дискретное косинусное преобразование (DCT):** К логарифмированным энергиям Мел-фильтров применяется DCT. Это похоже на обратное преобразование Фурье и позволяет перейти из "частотной" области (спектра) в "кепстральную" область. Цель — декоррелировать признаки (сделать их менее зависимыми друг от друга).
5.  **Выбор коэффициентов:** Из полученных кепстральных коэффициентов оставляют только первые 12-20. Низкочастотные коэффициенты несут информацию об основной форме спектра (тембре), а высокочастотные — о мелких деталях и шуме, которые часто можно отбросить.

**Резюме:**
*   **Мел-спектрограмма:** "Картинка", показывающая энергию звука во времени по частотам, адаптированным под человеческий слух. Хороша для **CNN**.
*   **MFCC:** Набор из нескольких чисел (вектор), который компактно описывает тембр звука в коротком временном окне. Хорош для классических моделей, полносвязных или **рекуррентных сетей (RNN)**.

Оба подхода являются мощными способами извлечения признаков из аудио, которые значительно улучшают качество моделей машинного обучения по сравнению с работой с "сырыми" аудиоданными.

## [25. DeepSets.](#Машинное-обучение-2025)

В предоставленных конспектах нет прямого упоминания термина "DeepSets". Однако в **Лекции 10. Алгебра и геометрия глубокого обучения** подробно разбирается очень близкая концепция — построение архитектур для **множеств (Set)**. Идеи, описанные в лекции, точно соответствуют принципам, лежащим в основе DeepSets.

Используя информацию из **Лекции 10**, ответ можно сформулировать так:

**DeepSets** — это архитектура нейронной сети, специально разработанная для работы с **множествами** — неупорядоченными наборами элементов. Ключевое свойство такой архитектуры — **перестановочная инвариантность (permutation invariance)**, то есть результат работы модели не должен зависеть от порядка, в котором ей подаются элементы множества.

Согласно **Лекции 10**, для построения функции, преобразующей множество в вектор (`Set(X) → Vec`), используется следующая общая схема, которая и лежит в основе DeepSets:

1.  **Преобразование элементов:** Каждый элемент множества `x` из `X` независимо преобразуется в векторное представление с помощью некоторой функции `φ` (например, полносвязной нейронной сети).
    $\text{Для каждого } x_i \in \text{Set}: \quad y_i = \varphi(x_i)$

2.  **Агрегация:** Полученные векторные представления всех элементов агрегируются (объединяются) с помощью перестановочно-инвариантной операции `ρ`, такой как сумма, среднее или максимум.
    $V_{agg} = \rho(\{y_1, y_2, \dots, y_n\}) = \sum_{i=1}^{n} y_i \quad (\text{например, сумма})$

3.  **Финальное преобразование:** Агрегированный вектор `V_{agg}` может быть дополнительно обработан другой функцией `ψ` (ещё одной нейронной сетью) для получения конечного результата.
    $\text{Result} = \psi(V_{agg})$

**Пример из Лекции 10:** Архитектура для множеств может быть представлена как **полносвязный граф**, где информация от всех узлов агрегируется. В лекции также отмечается, что механизм **самовнимания (self-attention)**, используемый в Трансформерах, является более сложным и мощным способом обработки множеств (`Set(X) → Set(Y)`), поскольку он позволяет элементам "взвешенно" взаимодействовать друг с другом перед агрегацией.

Таким образом, хотя сам термин "DeepSets" и не упомянут, **Лекция 10** полностью описывает его фундаментальный принцип: **преобразование → агрегация → преобразование** для обеспечения инвариантности к порядку элементов.

## [26. Передача сообщений и случайное блуждание.](#Машинное-обучение-2025)

В предоставленных конспектах эти понятия подробно разбираются в контексте **графовых нейронных сетей** в **Лекции 10. Алгебра и геометрия глубокого обучения**.

### Передача сообщений (Message Passing)

**Передача сообщений** — это фундаментальный механизм работы большинства современных графовых нейронных сетей (GNN). Идея заключается в том, что каждая вершина (узел) графа итеративно обновляет своё векторное представление (эмбеддинг), агрегируя информацию от своих соседей.

Процесс на одном шаге (в одном слое GNN) можно описать так:

1.  **Сбор "сообщений":** Для каждой вершины её соседи (включая её саму, что обеспечивается добавлением **self-loop** или петли к самой себе) генерируют "сообщения". Сообщение — это обычно просто векторное представление соседа, возможно, преобразованное некоторой функцией.
2.  **Агрегация сообщений:** Вершина собирает все входящие сообщения и агрегирует их в один вектор с помощью перестановочно-инвариантной функции (например, сумма, среднее, максимум).
3.  **Обновление состояния:** Собранный агрегированный вектор используется для обновления собственного состояния (эмбеддинга) вершины. Обычно это делается с помощью нейросетевого слоя (например, полносвязного).

Как отмечается в лекции, для передачи информации между удалёнными вершинами графа необходимо выполнить столько шагов message passing, сколько рёбер в кратчайшем пути между ними. Чтобы ускорить этот процесс, можно вводить **фиктивные рёбра** между дальними узлами.

### Случайное блуждание (Random Walk)

В **Лекции 10** **случайный обход (блуждание) графа** упоминается как один из способов представить сложную графовую структуру для обработки нейросетью. Идея в том, что мы можем сгенерировать множество последовательностей вершин, "прогуливаясь" по графу случайным образом.

*   **Представление:** Граф можно представить как множество таких последовательностей: `Graph ≈ Set(Seq(Node))`.
*   **Применение:** Полученные последовательности можно затем обрабатывать моделями, предназначенными для работы с последовательностями, например, **рекуррентными нейронными сетями (RNN)** или **трансформерами**. Это позволяет извлекать информацию о локальной структуре и связях в графе.

Этот подход особенно полезен для задач, где важен не только сам факт связи между вершинами, но и контекст, в котором они появляются (например, в задачах node embedding, таких как Node2Vec или DeepWalk, которые в конспекте не упомянуты, но основаны на этой идее).

**Итог:** **Передача сообщений** — это прямой, итеративный механизм обновления информации в графе, лежащий в основе архитектур GNN. **Случайное блуждание** — это метод сэмплирования, который превращает графовую структуру в набор последовательностей, которые затем можно обрабатывать другими типами моделей.

## [27. Самообучение.](#Машинное-обучение-2025)

Согласно **Лекции 11**, **самообучение (Self-Supervised Learning, SSL)** — это подход к машинному обучению, который позволяет извлекать информативные представления (признаки) из больших объёмов **неразмеченных данных**. Ключевая идея состоит в том, чтобы автоматически создавать метки для данных, формулируя так называемую **предварительную задачу (pretext task)**. Модель, решая эту задачу, вынуждена изучать глубинную структуру и семантику данных.

Процесс обычно состоит из двух этапов:
1.  **Предобучение (Pre-training):** Модель обучается на большой неразмеченной выборке, решая *pretext-задачу*. Например, предсказывая вырезанную часть изображения.
2.  **Дообучение (Fine-tuning):** Полученная модель (или её часть, например, кодировщик) используется для решения целевой, **последующей задачи (downstream task)**, такой как классификация или детекция, уже на небольшом количестве размеченных данных.

### Методы и подходы самообучения (из Лекции 11)

В конспекте выделены три основных направления самообучения:

#### 1. Автокодировщики (Autoencoders)

*   **Pretext Task:** Восстановить исходный объект с выхода.
*   **Идея:** Нейронная сеть состоит из **кодировщика (encoder)**, который сжимает вход в низкоразмерное "бутылочное горлышко" (bottleneck), и **декодировщика (decoder)**, который восстанавливает из этого сжатого представления исходный объект. Чтобы сеть не научилась просто копировать вход, на неё накладываются ограничения (например, малый размер bottleneck или регуляризация). Кодировщик в итоге учится создавать полезные, сжатые представления данных. Упоминаются **шумоподавляющие (Denoising)** и **разреженные (Sparse)** автокодировщики.

#### 2. Псевдо-задачи (Pretext Tasks)

*   **Идея:** Создание задач, где метку можно сгенерировать из самого объекта.
*   **Примеры для изображений:**
    *   **Предсказание поворота:** Модель должна определить, на какой угол (0, 90, 180, 270 градусов) было повёрнуто изображение.
    *   **Решение головоломки:** Изображение разрезается на части, перемешивается, и модель должна определить правильную перестановку.
    *   **Предсказание контекста:** Модели показывают центральный патч изображения, и она должна предсказать, какой из нескольких вариантов патчей является его соседом.
*   **Примеры для текста (Word2Vec):**
    *   **CBOW:** Предсказать слово по его контексту.
    *   **Skip-gram:** Предсказать контекст по центральному слову.

#### 3. Контрастное обучение (Contrastive Learning)

*   **Pretext Task:** Сблизить представления похожих объектов и развести представления разных.
*   **Идея:** Для одного и того же объекта создаются две или более аугментированные версии (например, два разных кропа одного изображения). Они формируют **"позитивную пару"**. **"Негативными парами"** являются аугментированные версии разных объектов. Модель (часто **сиамская сеть**) обучается с помощью функции потерь, такой как **Triplet Loss**, минимизировать расстояние между векторами позитивных пар и максимизировать расстояние между векторами негативных пар.
*   **Формула Triplet Loss:** $L(a, p, n) = \max(\text{dist}(a, p) - \text{dist}(a, n) + \epsilon, 0)$, где `a` — якорь, `p` — позитивный пример, `n` — негативный.
*   **Пример:** **CLIP** — модель, которая совместно обучается на парах (изображение, текст) с помощью контрастного подхода.

### Отличие от Self-Training

Важно не путать самообучение (self-supervised learning) с **self-training**, который описан в **Лекции 14**. Self-training — это итеративный процесс, где модель, обученная на размеченных данных, делает предсказания для неразмеченных, а затем наиболее уверенные из этих предсказаний добавляются в обучающую выборку с псевдо-метками. В отличие от SSL, здесь решается сразу целевая задача, а не предварительная.

## [28. Автокодировщик.](#Машинное-обучение-2025)

Согласно **Лекции 11**, **автокодировщик (Autoencoder)** — это тип нейронной сети, который используется для **обучения без учителя** с целью получения эффективных низкоразмерных представлений данных (извлечения признаков). Основная идея заключается в том, что сеть учится восстанавливать на выходе свой же вход, пропуская его через "узкое место" — слой с меньшей размерностью.

### Архитектура и принцип работы

Автокодировщик состоит из двух основных частей:

1.  **Кодировщик (Encoder):** Эта часть сети принимает на вход исходные данные `x` и сжимает их в низкоразмерное скрытое (латентное) представление, часто называемое кодом или вектором `c(x)`.
2.  **Декодировщик (Decoder):** Эта часть берёт сжатое представление `c(x)` и пытается восстановить из него исходные данные, генерируя на выходе `d(c(x))`.

Цель обучения — минимизировать **ошибку реконструкции** между входом и выходом, например, с помощью среднеквадратичной ошибки:
$\min ||d(c(x)) - x||$

### Ограничения для предотвращения тривиального копирования

Чтобы автокодировщик не научился просто копировать вход на выход (тождественное преобразование), на него накладываются специальные ограничения:

*   **Структурные ограничения:** Самый распространённый способ — это создание **"бутылочного горлышка" (bottleneck)**. Скрытый слой, в котором содержится код, имеет значительно меньшую размерность, чем входной и выходной слои. Такой автокодировщик называется **недополненным (undercomplete)**.

*   **Регуляризационные ограничения:** Даже если размерность скрытого слоя велика, можно добавить в функцию потерь регуляризатор, который накладывает ограничения на сами активации. Например, **L1-регуляризация** приводит к **разреженному (Sparse) автокодировщику**, где одновременно активна лишь малая часть нейронов в скрытом слое.
    $\text{Loss} = ||d(c(x)) - x|| + \tau \cdot L(c(x))$

### Вариации автокодировщиков

В **Лекции 11** упоминаются следующие вариации:

*   **Разреженный (Sparse Autoencoder):** Использует регуляризацию для получения разреженных кодов.
*   **Шумоподавляющий (Denoising Autoencoder):** На вход подаётся "испорченная" (зашумлённая) версия данных, а модель учится восстанавливать их исходную, "чистую" версию. Это заставляет модель изучать более робастные признаки.
*   **Сжимающий (Contractive Autoencoder):** Дополнительный член в функции потерь заставляет кодировщик быть менее чувствительным к малым изменениям на входе.
*   **Вариационный автокодировщик (VAE):** Это уже **генеративная модель**, а не просто инструмент для извлечения признаков. Как отмечается в **Лекции 12**, VAE учится кодировать данные не в один конкретный вектор, а в параметры вероятностного распределения (обычно гауссовского), из которого затем сэмплируется вектор для декодера. Это позволяет генерировать новые, не виденные ранее данные.

## [29. Контрастное обучение.](#Машинное-обучение-2025)

Согласно **Лекции 11**, **контрастное обучение (Contrastive Learning)** — это один из ключевых подходов в **самообучении (self-supervised learning)**. Его основная идея заключается в том, чтобы обучить модель создавать такие векторные представления (эмбеддинги), при которых **похожие** объекты будут близки друг к другу в этом векторном пространстве, а **непохожие** — далеки.

### Принцип работы

1.  **Создание пар:** Для обучения используются пары (или тройки) примеров:
    *   **Позитивная пара:** Это два примера, которые считаются "похожими". В самообучении это обычно две разные аугментации одного и того же объекта (например, два разных кропа, поворота или цветовых искажения одного изображения).
    *   **Негативная пара:** Это два "непохожих" примера, то есть аугментации двух разных объектов.

2.  **Обучение модели:** Модель (часто используется **сиамская сеть**, состоящая из двух идентичных подсетей с общими весами) пропускает эти примеры через себя, получая для каждого векторное представление.

3.  **Функция потерь:** Обучение ведётся с помощью специальной функции потерь, которая "притягивает" векторы позитивных пар и "расталкивает" векторы негативных. В лекции приводится пример **Triplet Loss (потеря на тройках)**:
    *   Для каждой тройки, состоящей из **якоря (anchor, `a`)**, **позитивного примера (positive, `p`)** и **негативного примера (negative, `n`)**, функция потерь выглядит так:
        $L(a, p, n) = \max(\text{dist}(a, p) - \text{dist}(a, n) + \epsilon, 0)$
    *   Эта функция штрафует модель, если расстояние от якоря до негативного примера (`dist(a, n)`) не превышает расстояние до позитивного (`dist(a, p)`) как минимум на заданный "зазор" `ε`.

### Применение

*   **Обучение на неразмеченных данных:** Контрастное обучение позволяет создавать мощные предобученные модели без необходимости в ручной разметке.
*   **Обучение на одном/нескольких примерах (One/Few-shot learning):** Этот подход идеально подходит для задач, где классов много, а примеров для каждого класса мало. Обученная контрастивно модель может эффективно классифицировать новые объекты, сравнивая их векторные представления с представлениями "эталонных" объектов (центроидов) каждого класса.
*   **Мультимодальное обучение:** В лекции приводится пример **CLIP (Contrastive Language-Image Pre-training)**, который использует контрастное обучение для совместного выравнивания векторных пространств изображений и их текстовых описаний. Это позволяет модели понимать связь между визуальной и текстовой информацией.

## [30. Word2Vec.](#Машинное-обучение-2025)

Согласно **Лекции 11**, **Word2Vec** — это набор моделей, предназначенных для получения **векторных представлений слов (word embeddings)**. Основная цель — представить слова в виде плотных, низкоразмерных векторов таким образом, чтобы эти векторы отражали семантические и контекстуальные отношения между словами. Это является значительным улучшением по сравнению с наивным **one-hot кодированием**, которое создаёт огромные, разреженные и семантически неинформативные векторы.

### Дистрибутивная гипотеза

В основе Word2Vec лежит **дистрибутивная гипотеза**, сформулированная Харрисом в 1954 году: "Слова, встречающиеся в похожих контекстах, имеют похожий смысл". Следовательно, модель учится предсказывать слово на основе его окружения или наоборот.

### Две основные архитектуры Word2Vec

1.  **CBOW (Continuous Bag of Words — Непрерывный мешок слов):**
    *   **Задача:** Предсказать центральное слово по окружающим его словам (контексту).
    *   **Принцип:** Модель берёт векторы слов контекста, усредняет их и пытается на основе этого усреднённого вектора предсказать пропущенное центральное слово.
    *   **Функция потерь:** $-\log P(\text{word}_i \mid \text{context}(\text{word}_i))$

2.  **Skip-gram:**
    *   **Задача:** Предсказать слова контекста по одному центральному слову.
    *   **Принцип:** Модель берёт вектор центрального слова и на его основе пытается предсказать каждое из слов в его окружении. Этот подход обычно даёт лучшие результаты для редких слов и на больших корпусах.
    *   **Функция потерь:** $-\log P(\text{context}(\text{word}_i) \mid \text{word}_i)$

### Оптимизации обучения

Обучение на всём словаре было бы очень затратным. Поэтому в Word2Vec применяются эффективные трюки:
*   **Подвыборка (Subsampling):** Часто встречающиеся слова (например, предлоги "в", "на") несут мало семантической информации и встречаются в очень разных контекстах. Их можно случайным образом отбрасывать во время обучения, чтобы ускорить процесс и улучшить качество векторов для более редких слов.
*   **Негативное сэмплирование (Negative Sampling):** Вместо того чтобы на каждом шаге обновлять веса для всего многотысячного словаря (вычисляя softmax), для каждой позитивной пары (слово, контекст) генерируется несколько **негативных** (случайных) примеров. Модель обучается отличать реальную пару от этих случайных "негативных" пар, что является гораздо более простой задачей бинарной классификации и значительно ускоряет обучение.

В результате обучения Word2Vec создаёт векторное пространство, где слова с похожим значением (например, "король" и "королева") или контекстом оказываются близко друг к другу.

## [31. Авторегрессионная генерация.](#Машинное-обучение-2025)

Согласно вашим конспектам, **авторегрессионная генерация** — это подход к созданию новых данных (например, текста, изображений или звука), при котором каждый последующий элемент последовательности генерируется на основе всех предыдущих, уже сгенерированных элементов. Этот принцип является основой **авторегрессионных моделей**.

Информация по этой теме содержится в нескольких лекциях:

*   **Лекция 12 "Генеративные модели"** описывает авторегрессионные модели как один из основных классов генеративных моделей.
*   **Лекция 7 "Рекуррентные сети"** и **Лекция 8 "Трансформеры"** рассматривают архитектуры, которые используются для такой генерации.

### Принцип работы

Основная идея авторегрессионной модели заключается в разложении совместной вероятности последовательности $x = (x_1, x_2, \dots, x_n)$ в произведение условных вероятностей с помощью **цепного правила**:
$p(x) = p(x_1) \cdot p(x_2|x_1) \cdot p(x_3|x_1, x_2) \cdots p(x_n|x_1, \dots, x_{n-1})$
$p(x) = \prod_{i=1}^{n} p(x_i | x_1, \dots, x_{i-1})$

Генерация новой последовательности происходит пошагово:
1.  Сначала генерируется первый элемент $x_1$.
2.  Затем на основе $x_1$ генерируется второй элемент $x_2$.
3.  На основе $x_1, x_2$ генерируется $x_3$, и так далее.
На каждом шаге модель предсказывает распределение вероятностей для следующего элемента, а затем из этого распределения выбирается конкретное значение.

### Примеры моделей и архитектур

*   **Для изображений (Лекция 12):**
    *   **PixelRNN:** Использует рекуррентную сеть (LSTM) для генерации изображения пиксель за пикселем, строка за строкой.
    *   **PixelCNN:** Использует свёрточные сети для той же задачи, что позволяет ускорить обучение, но генерация всё равно остаётся последовательной и медленной.

*   **Для текста (Лекции 7, 8):**
    *   **Рекуррентные сети (RNN/LSTM):** Классический подход, где на каждом шаге модель получает на вход предыдущее сгенерированное слово и своё скрытое состояние и предсказывает следующее слово.
    *   **GPT-подобные трансформеры (Лекция 8):** Современный стандарт. Эти модели используют **авторегрессионное (маскированное) внимание**, где каждый токен может "смотреть" только на предыдущие. Это позволяет обрабатывать весь предыдущий контекст параллельно (в отличие от RNN), но генерация нового токена всё равно происходит последовательно.

### Методы выбора следующего токена (Лекция 7)

Поскольку модель на каждом шаге выдаёт распределение вероятностей, выбор следующего элемента можно делать по-разному:
*   **Жадный поиск (Greedy Search):** Всегда выбирать самый вероятный токен. Часто приводит к скучным и повторяющимся результатам.
*   **Поиск по лучам (Beam Search):** На каждом шаге отслеживается несколько (`k`) наиболее вероятных последовательностей ("лучей"), что позволяет найти более оптимальную и разнообразную генерацию.
*   **Сэмплирование:** Выбор токена случайным образом из предсказанного распределения (возможно, с модификациями, такими как Temperature Scaling).

### Анализ авторегрессионных моделей (из Лекции 12)

| Преимущества | Недостатки |
| :--- | :--- |
| **Явное вычисление правдоподобия:** Модель позволяет точно рассчитать вероятность сгенерированного объекта (в отличие от GAN). | **Медленная генерация:** Из-за последовательной природы процесса генерация происходит очень медленно, шаг за шагом. |
| **Высокое качество образцов:** Часто генерируют очень детализированные и качественные результаты. | |

## [32. Beam Search.](#Машинное-обучение-2025)

Согласно **Лекции 7 "Рекуррентные сети"**, **Поиск по лучам (Beam Search)** — это алгоритм, используемый при генерации последовательностей (например, текста в машинном переводе или генеративных языковых моделях) для нахождения более оптимальной последовательности, чем при простом "жадном" подходе.

Вместо того чтобы на каждом шаге генерации выбирать только один, самый вероятный следующий элемент (как в жадном поиске), Beam Search поддерживает **несколько (`k`) наиболее вероятных гипотез (последовательностей)** на каждом шаге. Это число `k` называется **шириной луча (beam width)**.

### Как работает Beam Search?

1.  **На первом шаге** модель генерирует `k` самых вероятных первых элементов последовательности. Каждая из этих гипотез становится отдельным "лучом".

2.  **На каждом последующем шаге** для **каждого** из `k` текущих "лучей" (гипотез) модель предсказывает распределение вероятностей для следующего элемента. Рассматриваются все возможные продолжения для всех `k` лучей.

3.  Из всех этих возможных продолжений (в общем случае `k * |словарь|` вариантов) выбираются **новые `k` гипотез** с самой высокой суммарной вероятностью (или логарифмом вероятности). Старые гипотезы отбрасываются.

4.  Процесс повторяется до тех пор, пока не будет достигнут специальный "конец последовательности" (end-of-sequence) токен или не будет достигнута максимальная длина.

### Цель и преимущества

*   **Повышение качества генерации:** Жадный поиск может "застрять" в локальном оптимуме, выбрав на раннем шаге очень вероятный токен, который, однако, приведёт к неудачной последовательности в целом. Beam Search, исследуя несколько путей одновременно, с большей вероятностью найдет глобально более оптимальную и качественную последовательность.
*   **Баланс между качеством и скоростью:** Beam Search является компромиссом между жадным поиском (быстрым, но не всегда точным) и полным перебором всех возможных последовательностей (невозможным из-за комбинаторного взрыва). Увеличение `k` повышает качество, но и увеличивает вычислительные затраты.

Таким образом, Beam Search — это эвристический метод поиска, который позволяет значительно улучшить качество генерируемых последовательностей по сравнению с жадным подходом.

## [33. Вариационный автокодировщик.](#Машинное-обучение-2025)

Согласно **Лекции 12 "Генеративные модели"** и упоминанию в **Лекции 11**, **Вариационный автокодировщик (Variational Autoencoder, VAE)** — это **генеративная модель**, а не просто инструмент для извлечения признаков, как классический автокодировщик. Его основная цель — научиться генерировать новые, правдоподобные данные, похожие на те, что были в обучающей выборке.

### Ключевое отличие от классического автокодировщика

В то время как обычный автокодировщик (**Лекция 11**) учится сжимать входной объект в **один конкретный вектор** в скрытом пространстве, VAE учится отображать входной объект в **параметры вероятностного распределения** (обычно многомерного гауссовского). Как правило, кодировщик VAE предсказывает два вектора: вектор средних значений $\mu$ и вектор стандартных отклонений $\sigma$.

### Принцип работы VAE

1.  **Кодировщик (Encoder):** На вход подаётся объект `x`. Кодировщик (называемый `q_\phi(z|x)`) вычисляет параметры распределения в скрытом пространстве — векторы $\mu_x$ и $\sigma_x$.

2.  **Сэмплирование:** Из этого распределения $N(\mu_x, \sigma_x)$ случайным образом сэмплируется один вектор `z`. Чтобы этот процесс был дифференцируемым и можно было применять градиентный спуск, используется **трюк репараметризации (reparameterization trick)**: $z = \mu_x + \sigma_x \cdot \epsilon$, где $\epsilon$ — это случайный шум, взятый из стандартного нормального распределения $N(0, 1)$.

3.  **Декодировщик (Decoder):** Сэмплированный вектор `z` подаётся на вход декодеру (`p_\theta(x|z)`), который пытается восстановить из него исходный объект `x`.

### Обучение и функция потерь (ELBO)

Оптимизировать правдоподобие $p(x)$ напрямую в VAE сложно из-за неразрешимого интеграла. Поэтому вместо него максимизируется его **нижняя вариационная оценка (Evidence Lower Bound, ELBO)**. Функция потерь VAE (которую нужно минимизировать) состоит из двух частей:

1.  **Ошибка реконструкции:** Эта часть заставляет декодер генерировать объекты, похожие на исходные. Обычно это среднеквадратичная ошибка или перекрёстная энтропия.
    $E_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)]$

2.  **KL-дивергенция:** Эта часть выступает в роли регуляризатора. Она заставляет распределения, генерируемые кодировщиком (`q_\phi(z|x)`), быть похожими на априорное распределение в скрытом пространстве (обычно стандартное нормальное, $N(0, 1)$). Это делает скрытое пространство гладким и хорошо структурированным, что критически важно для генерации новых объектов.
    $D_{KL}(q_\phi(z|x) || p(z))$

### Анализ VAE (из Лекции 12)

| Плюсы | Минусы |
| :--- | :--- |
| **Фундаментальный подход:** Основан на строгой вероятностной теории. | **Размытые результаты:** Сгенерированные изображения часто получаются более "размытыми" и менее чёткими, чем у GAN. |
| **Осмысленное латентное пространство:** Позволяет получать и анализировать скрытые представления объектов. | **ELBO не всегда коррелирует с качеством:** Улучшение метрики ELBO не всегда означает улучшение визуального качества генерации. |

Для генерации нового объекта после обучения достаточно взять случайный вектор `z` из априорного распределения $N(0, 1)$ и подать его на вход декодеру.

## [34. Генеративно-состязательные сети.](#Машинное-обучение-2025)

Согласно **Лекции 12 "Генеративные модели"**, **Генеративно-состязательные сети (Generative Adversarial Networks, GAN)** — это класс моделей для обучения без учителя, основанный на идее состязания двух нейронных сетей: **Генератора** и **Дискриминатора**.

### Принцип работы

Процесс обучения GAN можно представить как игру между фальшивомонетчиком и экспертом:

1.  **Генератор (Generator, G):** Это "фальшивомонетчик". Он получает на вход случайный шум (вектор `z`) и пытается создать новые данные (например, изображения), которые были бы неотличимы от настоящих. Его цель — "обмануть" Дискриминатора.

2.  **Дискриминатор (Discriminator, D):** Это "эксперт". Он получает на вход как настоящие данные из обучающей выборки, так и "фейковые" данные, созданные Генератором. Его задача — правильно определить, какие данные настоящие, а какие — поддельные.

### Состязательное обучение

Обучение этих двух сетей происходит одновременно в рамках теоретико-игровой **минимаксной задачи**. Целевая функция, приведённая в лекции, выглядит так:
$\min_G \max_D V(D, G) = E_{x \sim p_{data}} [\log D(x)] + E_{z \sim p(z)} [\log(1 - D(G(z)))]$
*   **Дискриминатор (`max_D`)** стремится максимизировать эту функцию: он хочет присвоить настоящим данным (`D(x)`) вероятность, близкую к 1, а сгенерированным (`D(G(z))`) — близкую к 0.
*   **Генератор (`min_G`)** стремится минимизировать эту функцию: он пытается создать такие данные, чтобы Дискриминатор ошибся и присвоил им (`D(G(z))`) вероятность, близкую к 1.

На практике обучение происходит **поочерёдно**:
1.  Делается шаг оптимизации для Дискриминатора (обучаем его лучше отличать подделки).
2.  Делается шаг оптимизации для Генератора (обучаем его лучше обманывать Дискриминатор).

В лекции отмечается, что для стабилизации обучения на практике целевую функцию для генератора часто меняют на максимизацию `log(D(G(z)))`, чтобы избежать затухания градиентов на начальных этапах.

### Проблемы GAN

Лекция выделяет несколько ключевых проблем, с которыми сталкиваются при обучении GAN:
*   **Нестабильность обучения и отсутствие сходимости:** Нет гарантии, что сети достигнут равновесия; они могут осциллировать.
*   **Коллапс мод (Mode Collapse):** Генератор находит один или несколько удачных примеров, которые хорошо обманывают Дискриминатор, и начинает генерировать только их, игнорируя всё разнообразие данных.
*   **Нет чёткого критерия остановки:** Сложно понять, когда модель достаточно обучена.

### Интересные идеи и вариации GAN

*   **Conditional GAN (cGAN):** Позволяет управлять процессом генерации, подавая в Генератор и Дискриминатор дополнительную информацию (например, метку класса `y`). Это позволяет целенаправленно генерировать, например, "кошку" или "собаку".
*   **Wasserstein GAN (WGAN):** Использует **расстояние Вассерштайна** вместо KL-дивергенции в качестве метрики. Это улучшает стабильность обучения и помогает бороться с коллапсом мод.
*   **CycleGAN:** Позволяет преобразовывать изображения из одного домена в другой без необходимости в парных данных (например, преобразовывать фотографии лошадей в зебр).

### Сравнение с другими моделями

Лекция подчёркивает, что GAN особенно хороши в генерации **высококачественных (High Quality)**, чётких образцов, но могут страдать от недостаточного разнообразия (покрытия мод).

## [35. Диффузная модель.](#Машинное-обучение-2025)

Согласно **Лекции 12 "Генеративные модели"**, **диффузионные модели (Diffusion Models)** — это класс современных генеративных моделей, которые создают новые данные, постепенно "очищая" их из чистого шума. Этот подход в последние годы показал выдающиеся результаты, особенно в генерации высококачественных изображений.

### Суть метода

Процесс обучения и генерации в диффузионных моделях состоит из двух ключевых этапов:

1.  **Прямой процесс (Forward Process): Зашумление**
    *   Этот процесс не обучается, а является фиксированным.
    *   Берётся чистое изображение из обучающей выборки, и к нему **поэтапно добавляется** гауссовский шум.
    *   За большое количество шагов (например, 1000) изображение постепенно превращается в чистый, изотропный шум.

2.  **Обратный процесс (Reverse Process): Восстановление**
    *   Это и есть **обучаемый** процесс.
    *   Нейронная сеть (обычно архитектуры типа U-Net) учится **обращать этот процесс вспять**. На каждом шаге модель получает на вход зашумлённое изображение и предсказывает, какой именно шум был добавлен на этом шаге, чтобы его можно было "вычесть".
    *   Цель модели — научиться на каждом шаге делать один маленький шаг по "очистке" изображения, постепенно восстанавливая его из шума.

### Генерация новых данных

После обучения для генерации нового, ранее не виденного изображения, модель выполняет следующие действия:
1.  Берётся случайный тензор, полностью состоящий из гауссовского шума.
2.  Этот шум подаётся на вход обученной модели.
3.  Модель итеративно, шаг за шагом, "очищает" этот шум, пока не получится чистое, правдоподобное изображение.

### Ключевые особенности

*   **Кондиционирование:** Как отмечено в лекции, диффузионные модели позволяют **"гибко задавать условия на результат"**. Это означает, что в процесс восстановления можно легко добавлять дополнительную информацию, такую как текстовое описание (как в моделях DALL-E 2, Midjourney, Stable Diffusion), метку класса или другое изображение, чтобы управлять генерацией.
*   **Качество и разнообразие:** В сравнительной таблице "трилеммы" генеративного обучения из лекции диффузионные модели помещены в категорию, обеспечивающую лучшее **"покрытие мод / разнообразие" (Mode Coverage/Diversity)**. Это означает, что они хорошо справляются с генерацией разнообразных примеров и не страдают от "коллапса мод", как это часто бывает у GAN.
*   **Скорость генерации:** Главным недостатком диффузионных моделей является **медленная скорость генерации**. Поскольку процесс восстановления является итеративным и требует сотен или тысяч последовательных шагов, создание одного изображения занимает значительно больше времени по сравнению с GAN или VAE.

## [36. point- pair- и list-wise подходы к рекомендациям.](#Машинное-обучение-2025)

В предоставленных конспектах **point-wise, pair-wise и list-wise подходы** не упоминаются. Информация ниже основана на общих знаниях в области рекомендательных систем и информационного поиска.

---

Эти три подхода представляют собой разные способы формулировки задачи обучения для рекомендательных или ранжирующих систем. Они отличаются тем, на каком уровне они оценивают релевантность объектов: на уровне одного объекта, пары объектов или целого списка.

### 1. Point-wise подход (поточечный)

*   **Как работает:** Модель рассматривает **каждый объект (item) в отдельности** и пытается предсказать его релевантность для конкретного пользователя. Это, по сути, задача **регрессии** (предсказать точный рейтинг, например, от 1 до 5) или **классификации** (предсказать, кликнет пользователь или нет — 0/1).
*   **Вход:** Пара (пользователь, объект).
*   **Цель:** Предсказать числовое значение (рейтинг, вероятность клика).
*   **Пример:** Обучить модель, которая для пары `(User_A, Movie_X)` предскажет рейтинг 4.5.
*   **Преимущества:** Простота реализации, можно использовать стандартные алгоритмы регрессии/классификации.
*   **Недостатки:** **Игнорирует относительный порядок.** Модель не учится напрямую тому, что один объект лучше другого. Она может дать очень близкие предсказанные рейтинги для всех объектов, что затруднит их ранжирование.

### 2. Pair-wise подход (попарный)

*   **Как работает:** Модель учится на **парах объектов** и пытается предсказать, какой из двух объектов в паре более релевантен для пользователя. Это задача **бинарной классификации** на парах.
*   **Вход:** Тройка (пользователь, позитивный объект, негативный объект). Позитивный объект — это тот, с которым пользователь взаимодействовал (например, купил), а негативный — тот, который проигнорировал.
*   **Цель:** Научиться предсказывать, что P(позитивный > негативный) = 1.
*   **Пример:** Для пользователя `User_A`, который посмотрел фильм `Movie_X`, но не посмотрел `Movie_Y`, модель учится тому, что `Movie_X` > `Movie_Y`.
*   **Преимущества:** Напрямую оптимизирует **порядок ранжирования**, что ближе к реальной задаче рекомендации.
*   **Недостатки:** Игнорирует полный контекст списка. Рассматривает только локальные взаимоотношения между парами и может быть вычислительно затратным из-за большого количества возможных пар.

### 3. List-wise подход (посписочный)

*   **Как работает:** Модель рассматривает **весь список рекомендуемых объектов целиком** и пытается напрямую оптимизировать метрику качества этого списка (например, nDCG, MAP). Этот подход наиболее точно соответствует конечной цели — предоставить пользователю наилучший отсортированный список.
*   **Вход:** Пользователь и целый список объектов.
*   **Цель:** Оптимизировать функцию потерь, которая напрямую отражает качество всего списка.
*   **Пример:** Модель получает на вход `User_A` и 10 фильмов и учится располагать их в таком порядке, чтобы максимизировать метрику nDCG@10.
*   **Преимущества:** **Самый теоретически корректный подход**, так как напрямую оптимизирует конечную цель.
*   **Недостатки:** **Наиболее сложный** в реализации. Требует специальных функций потерь и может быть очень вычислительно дорогим.

### Сводная таблица

| Подход | Единица обучения | Задача | Цель | Сложность |
| :--- | :--- | :--- | :--- | :--- |
| **Point-wise** | Один объект | Регрессия/Классификация | Предсказать точный рейтинг | Низкая |
| **Pair-wise** | Пара объектов | Бинарная классификация | Предсказать правильный порядок для пары | Средняя |
| **List-wise** | Список объектов | Ранжирование | Оптимизировать метрику всего списка | Высокая |

## [37. Разложение матриц (ALS)](#Машинное-обучение-2025)

В предоставленных конспектах **явного упоминания алгоритма ALS (Alternating Least Squares)** нет. Однако в **Лекции 10. Алгебра и геометрия глубокого обучения** разбирается концепция **коллаборативной фильтрации**, которая тесно связана с матричным разложением и является основой для ALS.

Используя эту информацию и общие знания, можно дать следующий ответ:

---

**Разложение матриц (Matrix Factorization)** — это один из самых популярных подходов в **коллаборативной фильтрации** для построения рекомендательных систем. Основная идея заключается в том, чтобы представить большую и разреженную матрицу взаимодействий "пользователь-объект" в виде произведения двух матриц меньшей размерности: матрицы факторов пользователей и матрицы факторов объектов.

Как указано в **Лекции 10**, задача коллаборативной фильтрации — это работа с парами `User × Item`. Каждый пользователь и каждый объект представляются в виде суммы отдельных сущностей. Это и приводит к идее матричного разложения.

### Принцип работы

1.  **Исходная матрица (R):** У нас есть матрица `R` размера `m × n`, где `m` — количество пользователей, `n` — количество объектов. Элемент `R_ij` содержит рейтинг, который пользователь `i` поставил объекту `j`. Эта матрица обычно очень **разреженная**, так как большинство пользователей взаимодействовало лишь с малой долей объектов.

2.  **Разложение:** Мы хотим найти две матрицы:
    *   **Матрица пользователей (P):** размера `m × k`, где каждая строка — это вектор скрытых факторов (эмбеддинг), описывающий предпочтения пользователя `i`.
    *   **Матрица объектов (Q):** размера `n × k`, где каждая строка — это вектор скрытых факторов, описывающий характеристики объекта `j`.
    Здесь `k` — это размерность скрытого пространства (гиперпараметр), которая обычно гораздо меньше `m` и `n`.

3.  **Предсказание:** Предсказанный рейтинг `R^_ij` вычисляется как скалярное произведение вектора пользователя `i` и вектора объекта `j`:
    $\hat{R}_{ij} = p_i \cdot q_j^T$
    Вся матрица предсказаний `R^` получается произведением `P` и `Q^T`.

### Метод ALS (Alternating Least Squares — Поочерёдный метод наименьших квадратов)

**ALS** — это итеративный алгоритм для нахождения матриц `P` и `Q`. Поскольку одновременное нахождение обеих матриц является сложной невыпуклой задачей, ALS упрощает её, оптимизируя их поочерёдно:

1.  **Инициализация:** Матрицы `P` и `Q` инициализируются случайными значениями.
2.  **Итеративный процесс:**
    *   **Шаг 1: Фиксируем Q, оптимизируем P.** Считая матрицу объектов `Q` константой, задача нахождения `P` становится стандартной задачей наименьших квадратов, которая имеет аналитическое решение. Мы обновляем все векторы пользователей `p_i`.
    *   **Шаг 2: Фиксируем P, оптимизируем Q.** Теперь, считая обновлённую матрицу пользователей `P` константой, мы аналогичным образом решаем задачу наименьших квадратов для `Q` и обновляем все векторы объектов `q_j`.
3.  **Повторение:** Шаги 1 и 2 повторяются до тех пор, пока алгоритм не сойдётся (например, ошибка на валидационном наборе перестанет уменьшаться).

**Преимущества ALS:**
*   **Хорошая распараллеливаемость:** Вычисления для разных пользователей (на шаге 1) или разных объектов (на шаге 2) независимы и могут выполняться параллельно. Это делает ALS хорошо подходящим для больших распределённых систем (например, Apache Spark).
*   **Работа с неявными данными:** Существуют модификации ALS для обработки неявных обратных связей (например, просмотры, клики), а не только явных рейтингов.

## [38. Самообучение (self-training) и сообучение (co-training)](#Машинное-обучение-2025)

Согласно **Лекции 14 "Активное обучение и разметка данных"**, **самообучение (self-training)** и **сообучение (co-training)** — это два подхода к обучению на **частично размеченных данных**, когда у нас есть небольшое количество размеченных примеров (`D_l`) и большое количество неразмеченных (`D_u`).

### Самообучение (Self-training)

*   **Идея:** Использовать одну модель для итеративного расширения своего же обучающего набора.
*   **Процесс:**
    1.  Начальная модель обучается на небольшом размеченном наборе данных `D_l`.
    2.  Эта модель используется для предсказания меток для всех неразмеченных данных `D_u`.
    3.  Объекты из `D_u`, для которых модель сделала **наиболее уверенные** предсказания, добавляются в `D_l` вместе со своими "псевдо-метками".
    4.  Модель переобучается на этом расширенном наборе.
    5.  Шаги 2-4 повторяются.
*   **Ключевая особенность:** Используется **одна** модель и **один** набор признаков. Успех сильно зависит от качества первоначальной модели и её способности делать точные предсказания.

### Сообучение (Co-training)

*   **Идея:** Использовать две (или более) разные модели, которые обучают друг друга.
*   **Процесс:**
    1.  Требуется, чтобы данные имели **два независимых "взгляда" (views)** — два набора признаков, каждый из которых достаточен для классификации (например, для веб-страницы один "взгляд" — это текст на самой странице, а второй — текст анкоров ссылок, ведущих на неё).
    2.  Две разные модели (`h1` и `h2`) обучаются на своих наборах признаков на исходных размеченных данных `D_l`.
    3.  Каждая модель делает предсказания на неразмеченных данных `D_u`.
    4.  Модель `h1` выбирает неразмеченный пример, в котором она **наиболее уверена**, и добавляет его с псевдо-меткой в обучающий набор для модели `h2`.
    5.  Аналогично, модель `h2` добавляет свой самый уверенный пример в набор для `h1`.
    6.  Обе модели переобучаются и процесс повторяется.
*   **Ключевая особенность:** Модели "обогащают" друг друга информацией, используя разные и, в идеале, условно независимые источники данных.

### Сравнительная таблица

| Характеристика | Самообучение (Self-training) | Сообучение (Co-training) |
| :--- | :--- | :--- |
| **Количество моделей** | **Одна** | **Две** (или более) |
| **Количество "взглядов"**| **Один** набор признаков | **Два** независимых набора признаков |
| **Принцип работы** | Модель сама себе добавляет данные | Модели добавляют данные друг другу |
| **Критерий добавления** | Наиболее уверенные предсказания | Наиболее уверенные предсказания каждой модели |

**Важное замечание:** Важно не путать **самообучение (self-training)** из **Лекции 14** с **самообучением (self-supervised learning)** из **Лекции 11**.  **Self-supervised learning** — это обучение на *pretext-задачах* для извлечения представлений, а **self-training** — это итеративное добавление псевдо-меток для решения *целевой* задачи.

## [39. Активное обучение.](#Машинное-обучение-2025)

Согласно **Лекции 14**, **активное обучение (Active Learning)** — это сценарий машинного обучения, в котором алгоритм может интерактивно **запрашивать у "Оракула"** (обычно человека-эксперта) метки для тех объектов, которые, по его мнению, принесут наибольшую пользу для обучения.

### Сценарий и цель

Активное обучение применяется, когда:
*   Неразмеченных данных очень много.
*   Разметка данных — долгий и дорогой процесс.
*   Модель может обучаться гораздо быстрее, чем происходит ручная разметка.

**Цель:** Достичь максимального качества модели при **минимальном количестве обращений** к Оракулу, то есть с минимальными затратами на разметку.

### Стратегии выбора объектов для разметки

Ключевым элементом активного обучения является **стратегия заявок** — правило, по которому модель решает, какой следующий объект отправить на разметку. В лекции перечислены следующие основные стратегии:

1.  **Отбор по неуверенности (Uncertainty Sampling):**
    *   **Идея:** Выбрать объект, в ответе на который модель **менее всего уверена**.
    *   **Варианты:**
        *   Минимальная уверенность в предсказанном классе.
        *   Максимальная энтропия в распределении вероятностей по классам.
        *   Минимальный "отступ" (margin) между вероятностями двух самых вероятных классов.

2.  **Запрос по комитету (Query by Committee):**
    *   **Идея:** Использовать "комитет" из нескольких моделей (ансамбль). Выбирается тот объект, по поводу которого у моделей **наибольшие разногласия** в предсказаниях.

3.  **Ожидаемое изменение модели (Expected Model Change):**
    *   **Идея:** Выбрать объект, который, как ожидается, **сильнее всего изменит параметры модели** (например, вызовет самый большой градиент функции потерь).

4.  **Ожидаемое сокращение ошибки (Expected Error Reduction):**
    *   **Идея:** Выбрать объект, разметка которого, как ожидается, **максимально снизит будущую ошибку** модели на всём наборе неразмеченных данных. Это самый теоретически обоснованный, но и самый вычислительно сложный подход.

5.  **Сэмплирование с учётом плотности (Density-based Sampling):**
    *   **Идея:** Модифицировать другие стратегии (например, отбор по неуверенности), отдавая предпочтение объектам, находящимся в плотных регионах данных, и игнорируя выбросы или аномалии. Это помогает избежать траты бюджета разметки на нетипичные, неинформативные примеры.

Примером простого активного обучения является SVM в одномерном пространстве, где на каждом шаге для разметки выбирается точка, ближайшая к текущей разделяющей гиперплоскости, что аналогично бинарному поиску.

## [40. Задача о многоруком бандите.](#Машинное-обучение-2025)

Согласно **Лекции 15 "Обучение с подкреплением"**, **задача о многоруком бандите** — это классическая задача, которая является упрощённым частным случаем обучения с подкреплением (Reinforcement Learning, RL). В этой задаче агент должен сделать серию выборов из нескольких вариантов ("ручек" игрового автомата, или "бандита"), каждый из которых приносит случайную награду с неизвестным для агента распределением.

**Цель агента** — максимизировать суммарный выигрыш за определённое количество попыток.

### Дилемма исследования против использования (Exploration vs. Exploitation)

Эта задача идеально иллюстрирует ключевую дилемму в RL:
*   **Использование (Exploitation):** Агент выбирает ручку, которая на данный момент принесла ему наибольший средний выигрыш. Это максимизирует сиюминутную награду.
*   **Исследование (Exploration):** Агент пробует другие ручки, включая те, что пока показали себя не очень хорошо, чтобы собрать больше информации об их реальной доходности. Это может привести к потере сиюминутной награды, но в долгосрочной перспективе может помочь найти истинно оптимальную ручку.

### Стратегии решения (из Лекции 15)

1.  **Жадный алгоритм (Greedy):**
    *   **Принцип:** Сначала попробовать каждую ручку один раз, а затем всегда выбирать ту, у которой текущий средний выигрыш максимален.
    *   **Недостаток:** Алгоритм полностью полагается на **использование**. Если на начальном этапе оптимальная ручка случайно выдаст низкий результат, она может быть навсегда проигнорирована.

2.  **𝜀-жадный алгоритм (Epsilon-greedy):**
    *   **Принцип:** Это компромисс между исследованием и использованием. С вероятностью `1-ε` агент выбирает лучшую на данный момент ручку (использование), а с малой вероятностью `ε` — случайную ручку (исследование).
    *   **Недостаток:** Исследование происходит совершенно случайно и не учитывает, насколько "перспективными" могут быть другие ручки.

3.  **Softmax (или SoftArgMax):**
    *   **Принцип:** Более "умное" исследование. Вероятность выбора каждой ручки пропорциональна её текущей оценке среднего выигрыша. Ручки с более высокими оценками выбираются чаще, но и у ручек с низкими оценками остаётся шанс быть выбранными. Специальный параметр "температура" позволяет регулировать баланс: высокая температура делает выбор более случайным (больше исследования), низкая — более жадным.

4.  **Алгоритм Верхней доверительной границы (Upper Confidence Bound, UCB1):**
    *   **Принцип:** "Оптимизм в условиях неопределённости". Для каждой ручки вычисляется не только её средний выигрыш, но и "бонус за неопределённость", который тем выше, чем реже эту ручку дёргали. Алгоритм выбирает ручку с максимальным значением суммы среднего выигрыша и этого бонуса.
    *   **Формула метрики выбора:**
        $q_i + \lambda \cdot \sqrt{\frac{2\ln(n)}{n_i}}$
        где `q_i` — средний выигрыш, `n` — общее число попыток, `n_i` — число попыток для ручки `i`.

### Контекстные многорукие бандиты

В лекции также упоминается расширение этой задачи — **контекстные бандиты**, где выигрыш зависит не только от выбранной ручки, но и от текущего **состояния (контекста)**. Например, в рекомендательной системе "ручка" — это товар, который нужно порекомендовать, а "контекст" — это информация о пользователе. Здесь для каждой ручки уже нужно строить отдельную модель, предсказывающую выигрыш в зависимости от контекста.

## [41. Байесовская оптимизация.](#Машинное-обучение-2025)

Согласно **Лекции 15 "Обучение с подкреплением"**, **байесовская оптимизация** — это метод глобальной оптимизации "чёрного ящика", который используется для нахождения максимума (или минимума) функции, вычисление которой очень **дорогое или затратное по времени**. Классический пример — подбор гиперпараметров для модели машинного обучения, где каждая оценка функции означает полный цикл обучения и валидации модели.

В **Лекции 14** также упоминается, что **Гауссовский процесс**, используемый в байесовской оптимизации, служит **суррогатной функцией** для поиска экстремумов.

### Принцип работы

Вместо того чтобы напрямую работать с дорогой целевой функцией, байесовская оптимизация строит её дешёвую **суррогатную модель** (часто на основе Гауссовского процесса). Эта модель не только предсказывает значение функции в любой точке, но и оценивает **неопределённость** этого предсказания.

Алгоритм состоит из двух ключевых компонентов:

1.  **Суррогатная модель (Probabilistic Surrogate Model):**
    *   Аппроксимирует целевую функцию на основе уже вычисленных точек.
    *   Обычно для этого используется **Гауссовский процесс (GP)**, который для любой новой точки `x` может предсказать среднее значение `μ(x)` и дисперсию (неопределённость) `σ(x)`.

2.  **Функция приобретения (Acquisition Function):**
    *   На основе предсказаний суррогатной модели эта функция решает, какую следующую точку нужно вычислить. Она балансирует между **использованием (exploitation)** — выбором точек в областях, где суррогат предсказывает высокое значение, и **исследованием (exploration)** — выбором точек в областях с высокой неопределённостью.

### Популярные функции приобретения (из лекции 15)

*   **Upper Confidence Bound (UCB):** Выбирает точку, которая максимизирует верхнюю границу доверительного интервала. Формула аналогична той, что используется в задаче о многоруких бандитах:
    $\mu(x) + \lambda \cdot \sigma(x)$
*   **Probability of Improvement (PI):** Выбирает точку, которая с наибольшей вероятностью улучшит текущее лучшее найденное значение.
*   **Expected Improvement (EI):** Выбирает точку, которая даёт максимальное математическое ожидание улучшения по сравнению с текущим лучшим значением.

### Шаги алгоритма

1.  Вычисляется значение целевой функции в нескольких случайно выбранных точках.
2.  На основе этих точек строится (обучается) суррогатная модель.
3.  На каждом последующем шаге:
    a. Находится точка, максимизирующая функцию приобретения.
    b. В этой точке вычисляется реальное значение целевой функции.
    c. Новая точка добавляется в набор данных, и суррогатная модель обновляется.
4.  Процесс повторяется до исчерпания бюджета вычислений.

Таким образом, байесовская оптимизация позволяет эффективно находить оптимум дорогих функций за небольшое число вызовов.

## [42. Обучение с подкреплением.](#Машинное-обучение-2025)

Согласно **Лекции 15**, **Обучение с подкреплением (Reinforcement Learning, RL)** — это область машинного обучения, в которой **агент** учится взаимодействовать с **окружающей средой** для достижения цели. Агент принимает решения (выбирает **действия**), находясь в определённых **состояниях**, а среда в ответ предоставляет ему **награду** (положительную или отрицательную) и переводит в новое состояние.

**Основная цель агента** — выучить такую стратегию поведения (**политику**), которая максимизирует суммарную накопленную награду во времени.

### Ключевая дилемма: Исследование vs. Использование

В основе RL лежит фундаментальная дилемма **Exploration vs. Exploitation**:
*   **Использование (Exploitation):** Применение уже известных действий, которые приносят гарантированно высокую награду.
*   **Исследование (Exploration):** Проба новых, неизвестных действий с целью собрать больше информации о среде и, возможно, найти ещё более выгодную стратегию в будущем.
Эта дилемма хорошо иллюстрируется на примере **задачи о многоруком бандите**, где простые стратегии, такие как **𝜀-жадный алгоритм** или **UCB**, пытаются найти баланс между этими двумя крайностями.

### Формализация задачи (Марковский процесс принятия решений)

Задача RL формализуется с помощью следующих компонентов:
*   **Состояния (S):** Множество всех возможных ситуаций, в которых может находиться агент.
*   **Действия (A):** Множество всех возможных действий, которые агент может совершить.
*   **Функция перехода (P):** Вероятность `P(s'|s, a)` перехода в состояние `s'` из состояния `s` при совершении действия `a`.
*   **Функция награды (r):** Награда `r(s, a)`, получаемая агентом.
*   **Политика (π):** Стратегия агента `π(a|s)`, определяющая, какое действие `a` выбрать в состоянии `s`.

### Функции ценности и уравнения Беллмана

Для оценки "хорошести" состояний и действий вводятся **функции ценности**:
*   **Ценность состояния (V-function):** `V_π(s)` — ожидаемая суммарная дисконтированная награда, начиная из состояния `s` и следуя политике `π`.
*   **Ценность действия (Q-function):** `Q_π(s, a)` — ожидаемая суммарная дисконтированная награда, начиная с совершения действия `a` в состоянии `s` и далее следуя политике `π`.
    $R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k} \quad (\text{где } 0 < \gamma < 1 \text{ — дисконтирующий множитель})$

Эти функции связаны между собой **уравнениями Беллмана**. Уравнение Беллмана для Q-функции, приведённое в лекции, выглядит так:
$Q_\pi(s, a) = \sum_{s'} P(s' | s, a) [r(s, a) + \gamma V_\pi(s')]$
Оно рекурсивно определяет ценность текущего действия через немедленную награду и дисконтированную ценность следующего состояния.

### Q-learning

**Q-learning** — это один из фундаментальных алгоритмов RL, который позволяет агенту выучить оптимальную Q-функцию `Q*(s, a)`, даже не зная модели среды (функций перехода и награды).

*   **Принцип:** Алгоритм итеративно обновляет оценки в таблице `Q(s, a)` (Q-таблице), используя опыт, полученный от взаимодействия со средой.
*   **Формула обновления:**
    $Q(s, a) \leftarrow (1-\alpha)Q(s, a) + \alpha (r + \gamma \max_{a'} Q(s', a'))$
    Здесь `α` — это скорость обучения. Агент обновляет ценность пары `(s, a)` на основе полученной награды `r` и максимальной Q-ценности, достижимой из нового состояния `s'`.

### Проблемы и современные подходы

Простая Q-таблица работает только для сред с небольшим, конечным числом состояний. Для сложных задач с огромными или непрерывными пространствами состояний (например, игры Atari или робототехника) используются **аппроксиматоры** (чаще всего нейронные сети) для оценки Q-функции. Это привело к появлению **глубокого обучения с подкреплением (Deep Reinforcement Learning)**. В лекции также упоминаются более продвинутые подходы, такие как **Actor-Critic**, **Offline RL** и другие. Для практики и экспериментов используются среды, такие как **Gymnasium (бывший OpenAI Gym)**.

## [43. Уравнение Беллмана.](#Машинное-обучение-2025)

Согласно **Лекции 15 "Обучение с подкреплением"**, **уравнение Беллмана** — это фундаментальное соотношение в RL, которое рекурсивно связывает ценность (value) текущего состояния или действия с ценностью последующих состояний. Оно является основой для большинства алгоритмов обучения с подкреплением.

### Основная идея

Уравнение Беллмана формализует интуитивную идею о том, что "ценность сейчас" равна "немедленной награде" плюс "дисконтированная ценность будущего". Оно позволяет вычислять ценность состояний (`V-function`) и действий (`Q-function`) итеративно.

### Уравнение Беллмана для Q-функции

В лекции приводится уравнение Беллмана для **Q-функции (ценности действия)** при следовании некоторой политике `π`:
$Q_\pi(s, a) = \sum_{s'} P(s' | s, a) [r(s, a) + \gamma V_\pi(s')]$
Давайте разберём эту формулу:

*   `Q_π(s, a)`: Это ценность совершения действия `a` в состоянии `s` (и дальнейшего следования политике `π`).
*   `P(s' | s, a)`: Вероятность перехода в состояние `s'` после совершения действия `a` в состоянии `s`.
*   `Σ_{s'}`: Сумма по всем возможным следующим состояниям `s'`, что учитывает стохастичность (случайность) среды.
*   `r(s, a)`: **Немедленная награда**, которую агент получает за действие `a` в состоянии `s`.
*   `γ V_π(s')`: **Дисконтированная ценность будущего**. `γ` (гамма) — это дисконтирующий множитель (`0 < γ < 1`), который делает будущие награды менее ценными, чем немедленные. `V_π(s')` — это ценность следующего состояния `s'`.

### Уравнение оптимальности Беллмана

Особенно важным является **уравнение оптимальности Беллмана**, которое описывает ценность при следовании **оптимальной политике** (`π*`). Для оптимальной Q-функции (`Q*`) оно выглядит так:
$Q^*(s, a) = \sum_{s'} P(s' | s, a) [r(s, a) + \gamma \max_{a'} Q^*(s', a')]$
Здесь мы заменили `V_π(s')` на `max_{a'} Q*(s', a')`, так как в следующем состоянии оптимальная политика всегда выберет действие с максимальной Q-ценностью.

### Роль в алгоритмах RL

Это уравнение лежит в основе многих алгоритмов RL, включая **Q-learning**. Формула обновления в Q-learning является практической, итеративной реализацией уравнения оптимальности Беллмана:
$Q_{new}(s, a) \leftarrow (1-\alpha)Q_{old}(s, a) + \alpha (\underbrace{r + \gamma \max_{a'} Q_{old}(s', a')}_{\text{Цель Беллмана}})$
Здесь `r + γ max_{a'} Q(s', a')` — это "целевое значение" (Bellman target), к которому мы постепенно сдвигаем нашу текущую оценку `Q(s, a)`. Таким образом, алгоритм итеративно приближает Q-функцию к оптимальной, предсказанной уравнением Беллмана.

---

При помощи **Gemini 2.5 Pro** - формулировки и **ChatGPT-4o** - оцифровка презентаций

